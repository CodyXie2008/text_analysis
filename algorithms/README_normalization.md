# å½’ä¸€åŒ–ç®—æ³•ä½¿ç”¨æ–‡æ¡£

## ğŸ“‹ æ¦‚è¿°

æœ¬æ¨¡å—æä¾›äº†å¤šç§æ•°æ®å½’ä¸€åŒ–å’Œæ ‡å‡†åŒ–ç®—æ³•ï¼Œç”¨äºå°†ä¸åŒé‡çº²ã€ä¸åŒèŒƒå›´çš„æ•°å€¼æ˜ å°„åˆ°ç»Ÿä¸€çš„èŒƒå›´ï¼Œç¡®ä¿åœ¨è¿›è¡Œç»¼åˆè®¡ç®—æ—¶å…¬å¹³åœ°å¯¹å¾…æ¯ä¸€ä¸ªæŒ‡æ ‡ã€‚

## ğŸš€ æ”¯æŒçš„ç®—æ³•

### 1. Min-Maxå½’ä¸€åŒ– (MinMaxNormalizer)
**é€‚ç”¨åœºæ™¯**: æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€ï¼Œæ— å¼‚å¸¸å€¼
**å…¬å¼**: `X_norm = (X - X_min) / (X_max - X_min) * (max - min) + min`
**ç‰¹ç‚¹**: å°†æ•°æ®çº¿æ€§å˜æ¢åˆ°æŒ‡å®šèŒƒå›´ï¼Œä¿æŒåŸå§‹æ•°æ®åˆ†å¸ƒå½¢çŠ¶

### 2. Z-Scoreæ ‡å‡†åŒ– (ZScoreNormalizer)
**é€‚ç”¨åœºæ™¯**: æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
**å…¬å¼**: `Z = (X - Î¼) / Ïƒ`
**ç‰¹ç‚¹**: å°†æ•°æ®æ ‡å‡†åŒ–ä¸ºå‡å€¼ä¸º0ï¼Œæ ‡å‡†å·®ä¸º1çš„åˆ†å¸ƒ

### 3. ç¨³å¥ç¼©æ”¾ (RobustScaler)
**é€‚ç”¨åœºæ™¯**: æ•°æ®åŒ…å«å¼‚å¸¸å€¼
**å…¬å¼**: `X_scaled = (X - Q2) / (Q3 - Q1)`
**ç‰¹ç‚¹**: ä½¿ç”¨å››åˆ†ä½æ•°è¿›è¡Œç¼©æ”¾ï¼Œå¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ

### 4. å°æ•°ç¼©æ”¾ (DecimalScaler)
**é€‚ç”¨åœºæ™¯**: éœ€è¦æ§åˆ¶æ•°å€¼ç²¾åº¦
**å…¬å¼**: `X_scaled = X / scale_factor`
**ç‰¹ç‚¹**: å°†æ•°æ®ç¼©æ”¾åˆ°æŒ‡å®šçš„å°æ•°ä½æ•°

### 5. å¯¹æ•°å½’ä¸€åŒ– (LogNormalizer)
**é€‚ç”¨åœºæ™¯**: å³ååˆ†å¸ƒæ•°æ®
**å…¬å¼**: `X_log = log(X + Îµ)`
**ç‰¹ç‚¹**: å¯¹æ•°æ®è¿›è¡Œå¯¹æ•°å˜æ¢ï¼Œå‹ç¼©å¤§æ•°å€¼

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬ç”¨æ³•

#### 1. ä½¿ç”¨ç±»æ–¹æ³•ï¼ˆæ¨èï¼‰

```python
import numpy as np
from text_analysis.algorithms.normalization import MinMaxNormalizer

# åˆ›å»ºå½’ä¸€åŒ–å™¨
normalizer = MinMaxNormalizer(feature_range=(0, 1))

# æ‹Ÿåˆå¹¶å˜æ¢æ•°æ®
data = np.array([1, 5, 10, 15, 20])
normalized_data = normalizer.fit_transform(data)

# å¯¹æ–°æ•°æ®è¿›è¡Œå˜æ¢
new_data = np.array([8, 12, 18])
transformed_data = normalizer.transform(new_data)

# é€†å˜æ¢
original_data = normalizer.inverse_transform(transformed_data)
```

#### 2. ä½¿ç”¨ä¾¿æ·å‡½æ•°

```python
from text_analysis.algorithms.normalization import min_max_normalize

# ç›´æ¥å½’ä¸€åŒ–
data = np.array([1, 5, 10, 15, 20])
normalized_data = min_max_normalize(data, feature_range=(0, 1))
```

### è¯¦ç»†ç¤ºä¾‹

#### Min-Maxå½’ä¸€åŒ–ç¤ºä¾‹

```python
import numpy as np
import pandas as pd
from text_analysis.algorithms.normalization import MinMaxNormalizer

# ç¤ºä¾‹æ•°æ®
data = np.array([10, 25, 50, 75, 100])

# åˆ›å»ºå½’ä¸€åŒ–å™¨
normalizer = MinMaxNormalizer(feature_range=(0, 1))

# æ‹Ÿåˆå¹¶å˜æ¢
normalized = normalizer.fit_transform(data)
print(f"åŸå§‹æ•°æ®: {data}")
print(f"å½’ä¸€åŒ–ç»“æœ: {normalized.flatten()}")
# è¾“å‡º: [0.0, 0.16666667, 0.44444444, 0.72222222, 1.0]

# è‡ªå®šä¹‰èŒƒå›´
normalizer_custom = MinMaxNormalizer(feature_range=(-1, 1))
normalized_custom = normalizer_custom.fit_transform(data)
print(f"è‡ªå®šä¹‰èŒƒå›´ç»“æœ: {normalized_custom.flatten()}")
# è¾“å‡º: [-1.0, -0.66666667, -0.11111111, 0.44444444, 1.0]
```

#### Z-Scoreæ ‡å‡†åŒ–ç¤ºä¾‹

```python
from text_analysis.algorithms.normalization import ZScoreNormalizer

# ç¤ºä¾‹æ•°æ®
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])

# åˆ›å»ºæ ‡å‡†åŒ–å™¨
normalizer = ZScoreNormalizer()
standardized = normalizer.fit_transform(data)

print(f"åŸå§‹æ•°æ®: {data}")
print(f"æ ‡å‡†åŒ–ç»“æœ: {standardized.flatten()}")
print(f"æ ‡å‡†åŒ–åå‡å€¼: {np.mean(standardized):.6f}")  # æ¥è¿‘0
print(f"æ ‡å‡†åŒ–åæ ‡å‡†å·®: {np.std(standardized):.6f}")  # æ¥è¿‘1
```

#### ç¨³å¥ç¼©æ”¾ç¤ºä¾‹

```python
from text_analysis.algorithms.normalization import RobustScaler

# åŒ…å«å¼‚å¸¸å€¼çš„æ•°æ®
data = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 100])

# åˆ›å»ºç¨³å¥ç¼©æ”¾å™¨
scaler = RobustScaler()
scaled = scaler.fit_transform(data)

print(f"åŸå§‹æ•°æ®: {data}")
print(f"ç¨³å¥ç¼©æ”¾ç»“æœ: {scaled.flatten()}")
print(f"ç¼©æ”¾åä¸­ä½æ•°: {np.median(scaled):.6f}")  # æ¥è¿‘0
print(f"ç¼©æ”¾åIQR: {np.percentile(scaled, 75) - np.percentile(scaled, 25):.6f}")  # æ¥è¿‘1
```

#### å¯¹æ•°å½’ä¸€åŒ–ç¤ºä¾‹

```python
from text_analysis.algorithms.normalization import LogNormalizer

# å³ååˆ†å¸ƒæ•°æ®
data = np.array([1, 2, 5, 10, 50, 100, 500, 1000])

# åˆ›å»ºå¯¹æ•°å½’ä¸€åŒ–å™¨
normalizer = LogNormalizer(epsilon=1e-8, base=np.e)
log_normalized = normalizer.fit_transform(data)

print(f"åŸå§‹æ•°æ®: {data}")
print(f"å¯¹æ•°å½’ä¸€åŒ–ç»“æœ: {log_normalized.flatten()}")

# ä»¥10ä¸ºåº•çš„å¯¹æ•°
normalizer_10 = LogNormalizer(epsilon=1e-8, base=10)
log_normalized_10 = normalizer_10.fit_transform(data)
print(f"ä»¥10ä¸ºåº•çš„å¯¹æ•°å½’ä¸€åŒ–: {log_normalized_10.flatten()}")
```

### Pandasé›†æˆç¤ºä¾‹

```python
import pandas as pd
from text_analysis.algorithms.normalization import MinMaxNormalizer, ZScoreNormalizer

# åˆ›å»ºDataFrame
df = pd.DataFrame({
    'feature1': [1, 5, 10, 15, 20],
    'feature2': [100, 200, 300, 400, 500],
    'feature3': [0.1, 0.5, 1.0, 1.5, 2.0]
})

print("åŸå§‹DataFrame:")
print(df)

# Min-Maxå½’ä¸€åŒ–
normalizer = MinMaxNormalizer(feature_range=(0, 1))
normalized_df = normalizer.fit_transform(df)
print("\nMin-Maxå½’ä¸€åŒ–å:")
print(pd.DataFrame(normalized_df, columns=df.columns))

# Z-Scoreæ ‡å‡†åŒ–
z_normalizer = ZScoreNormalizer()
z_normalized_df = z_normalizer.fit_transform(df)
print("\nZ-Scoreæ ‡å‡†åŒ–å:")
print(pd.DataFrame(z_normalized_df, columns=df.columns))
```

## ğŸ”§ ä¾¿æ·å‡½æ•°

æ‰€æœ‰ç®—æ³•éƒ½æä¾›äº†ä¾¿æ·å‡½æ•°ï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ï¼š

```python
from text_analysis.algorithms.normalization import (
    min_max_normalize,
    z_score_normalize,
    robust_scale,
    decimal_scale,
    log_normalize
)

# ä¾¿æ·å‡½æ•°ä½¿ç”¨
data = np.array([1, 5, 10, 15, 20])

# Min-Maxå½’ä¸€åŒ–
normalized = min_max_normalize(data, feature_range=(0, 1))

# Z-Scoreæ ‡å‡†åŒ–
standardized = z_score_normalize(data)

# ç¨³å¥ç¼©æ”¾
scaled = robust_scale(data)

# å°æ•°ç¼©æ”¾
decimal_scaled = decimal_scale(data, n_decimals=2)

# å¯¹æ•°å½’ä¸€åŒ–
log_normalized = log_normalize(data, epsilon=1e-8, base=np.e)
```

## ğŸ“Š ç®—æ³•é€‰æ‹©æŒ‡å—

### é€‰æ‹©Min-Maxå½’ä¸€åŒ–çš„æƒ…å†µï¼š
- âœ… æ•°æ®åˆ†å¸ƒç›¸å¯¹å‡åŒ€
- âœ… æ— å¼‚å¸¸å€¼æˆ–å¼‚å¸¸å€¼è¾ƒå°‘
- âœ… éœ€è¦å°†æ•°æ®æ˜ å°„åˆ°ç‰¹å®šèŒƒå›´ï¼ˆå¦‚[0,1]ï¼‰
- âœ… éœ€è¦ä¿æŒé›¶å€¼çš„ä½ç½®

### é€‰æ‹©Z-Scoreæ ‡å‡†åŒ–çš„æƒ…å†µï¼š
- âœ… æ•°æ®è¿‘ä¼¼æ­£æ€åˆ†å¸ƒ
- âœ… å¯¹å¼‚å¸¸å€¼ä¸æ•æ„Ÿ
- âœ… éœ€è¦åŸºäºå‡å€¼å’Œæ ‡å‡†å·®çš„ç»Ÿè®¡ç‰¹æ€§
- âœ… åç»­ç®—æ³•å¯¹æ•°æ®åˆ†å¸ƒæœ‰ç‰¹å®šè¦æ±‚

### é€‰æ‹©ç¨³å¥ç¼©æ”¾çš„æƒ…å†µï¼š
- âœ… æ•°æ®åŒ…å«å¼‚å¸¸å€¼
- âœ… éœ€è¦åŸºäºä¸­ä½æ•°å’Œå››åˆ†ä½æ•°çš„ç»Ÿè®¡ç‰¹æ€§
- âœ… å¯¹å¼‚å¸¸å€¼æ•æ„Ÿçš„åœºæ™¯

### é€‰æ‹©å°æ•°ç¼©æ”¾çš„æƒ…å†µï¼š
- âœ… éœ€è¦æ§åˆ¶æ•°å€¼ç²¾åº¦
- âœ… æ•°æ®èŒƒå›´å·®å¼‚å¾ˆå¤§
- âœ… éœ€è¦ä¿æŒæ•°å€¼çš„ç›¸å¯¹å…³ç³»

### é€‰æ‹©å¯¹æ•°å½’ä¸€åŒ–çš„æƒ…å†µï¼š
- âœ… æ•°æ®å‘ˆå³ååˆ†å¸ƒ
- âœ… æ•°å€¼èŒƒå›´å·®å¼‚å¾ˆå¤§
- âœ… éœ€è¦å‹ç¼©å¤§æ•°å€¼çš„å½±å“

## âš ï¸ æ³¨æ„äº‹é¡¹

1. **æ‹Ÿåˆé¡ºåº**: å¿…é¡»å…ˆè°ƒç”¨`fit()`æ–¹æ³•ï¼Œå†è°ƒç”¨`transform()`æ–¹æ³•
2. **æ•°æ®æ ¼å¼**: æ”¯æŒnumpyæ•°ç»„ã€åˆ—è¡¨ã€pandas Serieså’ŒDataFrame
3. **é™¤é›¶å¤„ç†**: æ‰€æœ‰ç®—æ³•éƒ½åŒ…å«é™¤é›¶ä¿æŠ¤æœºåˆ¶
4. **é€†å˜æ¢**: åªæœ‰æ‹Ÿåˆåçš„å½’ä¸€åŒ–å™¨æ‰èƒ½è¿›è¡Œé€†å˜æ¢
5. **å¼‚å¸¸å€¼**: ç¨³å¥ç¼©æ”¾å’Œå¯¹æ•°å½’ä¸€åŒ–å¯¹å¼‚å¸¸å€¼æ›´é²æ£’

## ğŸ§ª æµ‹è¯•éªŒè¯

è¿è¡Œæµ‹è¯•è„šæœ¬éªŒè¯ç®—æ³•åŠŸèƒ½ï¼š

```bash
cd text_analysis/algorithms
python test_normalization.py
```

æµ‹è¯•åŒ…æ‹¬ï¼š
- âœ… æ‰€æœ‰ç®—æ³•çš„åŸºæœ¬åŠŸèƒ½
- âœ… é€†å˜æ¢çš„æ­£ç¡®æ€§
- âœ… ä¾¿æ·å‡½æ•°çš„ä¸€è‡´æ€§
- âœ… Pandasé›†æˆ
- âœ… é”™è¯¯å¤„ç†æœºåˆ¶

## ğŸ“ˆ æ€§èƒ½ç‰¹ç‚¹

| ç®—æ³• | æ—¶é—´å¤æ‚åº¦ | ç©ºé—´å¤æ‚åº¦ | å¯¹å¼‚å¸¸å€¼æ•æ„Ÿåº¦ |
|------|------------|------------|----------------|
| Min-Max | O(n) | O(1) | é«˜ |
| Z-Score | O(n) | O(1) | é«˜ |
| ç¨³å¥ç¼©æ”¾ | O(n log n) | O(1) | ä½ |
| å°æ•°ç¼©æ”¾ | O(n) | O(1) | ä¸­ |
| å¯¹æ•°å½’ä¸€åŒ– | O(n) | O(1) | ä½ |

## ğŸ”— ç›¸å…³é“¾æ¥

- [å½’ä¸€åŒ–ç®—æ³•æºç ](../normalization.py)
- [æµ‹è¯•è„šæœ¬](../test_normalization.py)
- [ç»Ÿè®¡ç®—æ³•æ€»è§ˆ](../README.md)
