#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†ææ¨¡å—
åŸºäºé˜¿é‡Œäº‘é€šç”¨æ–‡æœ¬å‘é‡APIå®ç°çˆ¶å­è¯„è®ºç›¸ä¼¼åº¦åˆ†æ
ç”¨äºè¯†åˆ«"æ¨¡ä»¿æ€§è¯„è®º"ï¼Œé‡åŒ–ä»ä¼—å¿ƒç†
"""

import os
import sys
import json
import time
import warnings
import argparse
from datetime import datetime
from typing import List, Dict, Tuple, Optional
import numpy as np
import pandas as pd
import requests
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.insert(0, project_root)

# å¯¼å…¥åŸºç¡€åˆ†æå™¨
from text_analysis.core.base_analyzer import BaseAnalyzer
from text_analysis.core.data_paths import PROJECT_ROOT, resolve_latest_cleaned_data

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
plt.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')


class AliyunTextVectorClient:
    """é˜¿é‡Œäº‘è¯å‘é‡APIå®¢æˆ·ç«¯"""
    
    def __init__(self, access_key_id: str, access_key_secret: str):
        self.access_key_id = access_key_id
        self.access_key_secret = access_key_secret
        self.endpoint = "https://alinlp.cn-hangzhou.aliyuncs.com"
        self.service_code = "alinlp"
        
    def get_text_vector(self, text: str) -> List[float]:
        """è·å–æ–‡æœ¬å‘é‡ - ä½¿ç”¨é˜¿é‡Œäº‘è¯å‘é‡API"""
        try:
            # å°è¯•ä½¿ç”¨é˜¿é‡Œäº‘SDK
            try:
                from aliyunsdkcore.client import AcsClient
                from aliyunsdkcore.request import CommonRequest
                
                # åˆ›å»ºAcsClientå®ä¾‹
                client = AcsClient(
                    self.access_key_id,
                    self.access_key_secret,
                    'cn-hangzhou'
                )
                
                # ä½¿ç”¨CommonRequest
                request = CommonRequest()
                request.set_domain('alinlp.cn-hangzhou.aliyuncs.com')
                request.set_version('2020-06-29')
                request.set_action_name('GetWeChGeneral')
                request.add_query_param('ServiceCode', 'alinlp')
                request.add_query_param('Text', text)
                request.add_query_param('Size', '100')
                request.add_query_param('Type', 'word')
                request.add_query_param('Operation', 'average')
                
                # å‘é€è¯·æ±‚
                response = client.do_action_with_exception(request)
                result = json.loads(response)
                
                # è§£æè¿”å›ç»“æœ
                if "Data" in result:
                    data_content = result["Data"]
                    if isinstance(data_content, str):
                        data_content = json.loads(data_content)
                    
                    if "result" in data_content and data_content["result"]:
                        if isinstance(data_content["result"], dict) and "vec" in data_content["result"]:
                            return data_content["result"]["vec"]
                        elif isinstance(data_content["result"], list):
                            vectors = [item["vec"] for item in data_content["result"] if "vec" in item]
                            if vectors:
                                import numpy as np
                                return np.mean(vectors, axis=0).tolist()
                
                raise Exception(f"APIè¿”å›æ•°æ®æ ¼å¼å¼‚å¸¸: {result}")
                
            except ImportError:
                raise Exception("é˜¿é‡Œäº‘SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install aliyun-python-sdk-core")
            except Exception as e:
                raise e
                
        except Exception as e:
            error_msg = str(e)
            if "400" in error_msg:
                if "BasicServiceNotActivated" in error_msg:
                    print("âš ï¸ é˜¿é‡Œäº‘NLPåŸºç¡€ç‰ˆæœåŠ¡æœªå¼€é€šï¼Œè¯·è®¿é—®ï¼šhttps://common-buy.aliyun.com/?commodityCode=nlp%5FalinlpBasePost%5Fpublic%5Fcn#/buy")
                elif "UserStatusInvalid" in error_msg:
                    print("âš ï¸ ç”¨æˆ·çŠ¶æ€æ— æ•ˆï¼Œè¯·æ£€æŸ¥è´¦æˆ·æ˜¯å¦æ¬ è´¹")
                elif "InvalidParameter" in error_msg:
                    print("âš ï¸ å‚æ•°æ— æ•ˆï¼Œè¯·æ£€æŸ¥APIè°ƒç”¨å‚æ•°")
                else:
                    print(f"âš ï¸ é˜¿é‡Œäº‘APIè°ƒç”¨å¤±è´¥(400é”™è¯¯): {error_msg}")
            else:
                print(f"âš ï¸ é˜¿é‡Œäº‘APIè°ƒç”¨å¤±è´¥: {error_msg}")
            
            print("ğŸ”„ è‡ªåŠ¨åˆ‡æ¢åˆ°æœ¬åœ°TF-IDFæ–¹æ³•...")
            # ä½¿ç”¨æœ¬åœ°TF-IDFä½œä¸ºå¤‡é€‰æ–¹æ¡ˆ
            return self._get_tfidf_vector(text)
    
    def _get_tfidf_vector(self, text: str) -> List[float]:
        """ä½¿ç”¨æœ¬åœ°TF-IDFè®¡ç®—æ–‡æœ¬å‘é‡"""
        try:
            from sklearn.feature_extraction.text import TfidfVectorizer
            import jieba
            
            # ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†
            text = text.strip()
            if not text:
                return [0.0] * 100
            
            # ä½¿ç”¨jiebaåˆ†è¯
            words = list(jieba.cut(text))
            processed_text = ' '.join(words)
            
            # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
            vectorizer = TfidfVectorizer(
                max_features=100,
                stop_words=None,
                ngram_range=(1, 2)
            )
            
            # è®¡ç®—TF-IDFå‘é‡
            tfidf_matrix = vectorizer.fit_transform([processed_text])
            vector = tfidf_matrix.toarray()[0].tolist()
            
            # ç¡®ä¿å‘é‡é•¿åº¦ä¸º100
            if len(vector) < 100:
                vector.extend([0.0] * (100 - len(vector)))
            elif len(vector) > 100:
                vector = vector[:100]
            
            return vector
            
        except Exception as e:
            print(f"âŒ TF-IDFè®¡ç®—å¤±è´¥: {e}")
            return [0.0] * 100
    
    def batch_get_vectors(self, texts: List[str], batch_size: int = 50, concurrency: int = 4, throttle_ms: int = 0) -> List[List[float]]:
        """æ‰¹é‡è·å–æ–‡æœ¬å‘é‡ï¼ˆæ”¯æŒå¹¶å‘ä¸èŠ‚æµï¼‰"""
        from concurrent.futures import ThreadPoolExecutor, as_completed
        total = len(texts)
        vectors: List[List[float]] = [None] * total  # type: ignore

        def _submit_range(start: int, end: int):
            print(f"ğŸ”„ å¤„ç†å‘é‡ {start+1}-{end}/{total}")
            with ThreadPoolExecutor(max_workers=max(1, concurrency)) as ex:
                futures = {}
                for idx in range(start, end):
                    if throttle_ms > 0 and (idx - start) % concurrency == 0:
                        time.sleep(throttle_ms / 1000.0)
                    futures[ex.submit(self.get_text_vector, texts[idx])] = idx
                for fut in as_completed(futures):
                    idx = futures[fut]
                    try:
                        vectors[idx] = fut.result()
                    except Exception:
                        vectors[idx] = [0.0] * 100

        for i in range(0, total, batch_size):
            _submit_range(i, min(i + batch_size, total))

        return vectors  # type: ignore


class SimilarityAnalyzer:
    """æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå™¨"""
    
    def __init__(self, 
                 similarity_threshold: float = 0.7,
                 time_diff_threshold: int = 3600,
                 min_text_length: int = 5,
                 vector_batch_size: int = 50,
                 vector_concurrency: int = 4,
                 vector_throttle_ms: int = 0):
        self.similarity_threshold = similarity_threshold
        self.time_diff_threshold = time_diff_threshold
        self.min_text_length = min_text_length
        self.vector_batch_size = vector_batch_size
        self.vector_concurrency = vector_concurrency
        self.vector_throttle_ms = vector_throttle_ms
        
        # åˆå§‹åŒ–é˜¿é‡Œäº‘å®¢æˆ·ç«¯
        access_key_id = os.getenv('NLP_AK_ENV')
        access_key_secret = os.getenv('NLP_SK_ENV')
        
        if not access_key_id or not access_key_secret:
            raise ValueError("âŒ è¯·è®¾ç½®é˜¿é‡Œäº‘APIå¯†é’¥ç¯å¢ƒå˜é‡: NLP_AK_ENV, NLP_SK_ENV")
        
        self.vector_client = AliyunTextVectorClient(access_key_id, access_key_secret)
        print("âœ… é˜¿é‡Œäº‘æ–‡æœ¬å‘é‡å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
        print(f"å‘é‡è¯·æ±‚å‚æ•°ï¼šå¹¶å‘={self.vector_concurrency} æ‰¹å¤§å°={self.vector_batch_size} é™æµ={self.vector_throttle_ms}ms")
    
    def _load_from_database(self, video_id: Optional[str] = None, limit: Optional[int] = None) -> pd.DataFrame:
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        try:
            from config.db_config import get_db_conn
            conn = get_db_conn()
            
            if video_id:
                # åˆ†ææŒ‡å®šè§†é¢‘çš„è¯„è®º
                sql = """
                SELECT comment_id as id, content, create_time, like_count, aweme_id
                FROM douyin_aweme_comment
                WHERE content IS NOT NULL AND LENGTH(content) > 5 AND aweme_id = %s
                ORDER BY create_time DESC
                """
                params = [video_id]
                print(f"åˆ†æè§†é¢‘ {video_id} çš„è¯„è®º...")
            else:
                # åˆ†ææ‰€æœ‰è¯„è®º
                sql = """
                SELECT comment_id as id, content, create_time, like_count, aweme_id
                FROM douyin_aweme_comment
                WHERE content IS NOT NULL AND LENGTH(content) > 5
                ORDER BY create_time DESC
                """
                params = []
                print("åˆ†ææ‰€æœ‰è¯„è®º...")
            
            if limit:
                sql += f" LIMIT {limit}"
                print(f"é™åˆ¶åˆ†ææ•°é‡: {limit}")
            
            df = pd.read_sql_query(sql, conn, params=params)
            conn.close()
            
            print(f"âœ… æˆåŠŸåŠ è½½ {len(df)} æ¡è¯„è®º")
            return df
            
        except Exception as e:
            print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def analyze(self, df: pd.DataFrame) -> Dict:
        """æ‰§è¡Œç›¸ä¼¼åº¦åˆ†æ"""
        print("=== å¼€å§‹æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æ ===")
        
        # 1. æ•°æ®é¢„å¤„ç†
        print("1. æ•°æ®é¢„å¤„ç†...")
        df_processed = self._preprocess_data(df)
        
        if df_processed.empty:
            return {"error": "æ²¡æœ‰æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œåˆ†æ"}
        
        # 2. è®¡ç®—æ–‡æœ¬å‘é‡
        print("2. è®¡ç®—æ–‡æœ¬å‘é‡...")
        vectors = self._calculate_text_vectors(df_processed['content'].tolist())
        
        # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        print("3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
        similarity_matrix = self._calculate_similarity_matrix(vectors)
        
        # 4. è¯†åˆ«è¯„è®ºå¯¹
        print("4. è¯†åˆ«è¯„è®ºå¯¹...")
        comment_pairs = self._identify_comment_pairs(df_processed)
        
        # 5. åˆ†æç›¸ä¼¼åº¦
        print("5. åˆ†æç›¸ä¼¼åº¦...")
        similarity_results = self._analyze_similarity(df_processed, similarity_matrix, comment_pairs)
        
        # 6. è¯†åˆ«æ¨¡ä»¿æ€§è¯„è®º
        print("6. è¯†åˆ«æ¨¡ä»¿æ€§è¯„è®º...")
        imitative_comments = self._identify_imitative_comments(df_processed, similarity_results)
        
        # 7. ç”Ÿæˆç»Ÿè®¡ç»“æœ
        print("7. ç”Ÿæˆç»Ÿè®¡ç»“æœ...")
        stats = self._generate_statistics(df_processed, similarity_results, imitative_comments)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        self.df_processed = df_processed
        self.similarity_results = similarity_results
        self.imitative_comments = imitative_comments
        
        print("âœ… æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå®Œæˆ")
        return stats
    
    def _preprocess_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ•°æ®é¢„å¤„ç†"""
        # å¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
        df_processed = df.copy()
        
        # è¿‡æ»¤ç©ºå†…å®¹å’ŒçŸ­å†…å®¹
        df_processed = df_processed[df_processed['content'].notna()]
        df_processed = df_processed[df_processed['content'].str.len() >= self.min_text_length]
        
        # å»é‡
        df_processed = df_processed.drop_duplicates(subset=['content'])
        
        print(f"   - åŸå§‹æ•°æ®: {len(df)} æ¡")
        print(f"   - æœ‰æ•ˆæ•°æ®: {len(df_processed)} æ¡")
        
        return df_processed
    
    def _calculate_text_vectors(self, texts: List[str]) -> List[List[float]]:
        """è®¡ç®—æ–‡æœ¬å‘é‡ï¼ˆå»é‡+å¹¶å‘ï¼‰"""
        # å»é‡æ˜ å°„
        unique_index: Dict[str, int] = {}
        order_to_text: Dict[int, str] = {}
        for i, t in enumerate(texts):
            if t not in unique_index:
                unique_index[t] = len(unique_index)
            order_to_text[i] = t
        unique_list = [None] * len(unique_index)
        for t, u in unique_index.items():
            unique_list[u] = t

        # ä½¿ç”¨é…ç½®çš„æ‰¹å¤§å°ã€å¹¶å‘ä¸é™æµï¼ˆé»˜è®¤é™æµ=0 å³ä¸åšä»»ä½•é™æµï¼‰
        vectors_unique = self.vector_client.batch_get_vectors(
            unique_list,
            batch_size=self.vector_batch_size,
            concurrency=self.vector_concurrency,
            throttle_ms=self.vector_throttle_ms,
        )
        vec_map: Dict[str, List[float]] = {t: vectors_unique[u] for t, u in unique_index.items()}
        return [vec_map[order_to_text[i]] for i in range(len(texts))]
    
    def _calculate_similarity_matrix(self, vectors: List[List[float]]) -> np.ndarray:
        """è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ"""
        vectors_array = np.array(vectors)
        similarity_matrix = cosine_similarity(vectors_array)
        return similarity_matrix
    
    def _identify_comment_pairs(self, df: pd.DataFrame) -> List[Tuple[int, int]]:
        """è¯†åˆ«è¯„è®ºå¯¹ï¼ˆç”¨äºç›¸ä¼¼åº¦åˆ†æï¼‰"""
        pairs = []
        n = len(df)
        
        # ä¸ºäº†é¿å…è®¡ç®—é‡è¿‡å¤§ï¼Œåªåˆ†æç›¸é‚»çš„è¯„è®ºå¯¹
        for i in range(n - 1):
            pairs.append((i, i + 1))
        
        print(f"   - æ‰¾åˆ° {len(pairs)} å¯¹ç›¸é‚»è¯„è®º")
        return pairs
    
    def _analyze_similarity(self, df: pd.DataFrame, similarity_matrix: np.ndarray, 
                          comment_pairs: List[Tuple[int, int]]) -> pd.DataFrame:
        """åˆ†æç›¸ä¼¼åº¦"""
        results = []
        
        for idx1, idx2 in comment_pairs:
            similarity = similarity_matrix[idx1][idx2]
            
            # è®¡ç®—æ—¶é—´å·®
            time1 = pd.to_datetime(df.iloc[idx1]['create_time'])
            time2 = pd.to_datetime(df.iloc[idx2]['create_time'])
            time_diff = abs((time2 - time1).total_seconds())
            
            # å¤„ç†ä¸åŒçš„å­—æ®µå
            id_field = 'id' if 'id' in df.columns else 'comment_id'
            
            results.append({
                'comment1_id': df.iloc[idx1][id_field],
                'comment2_id': df.iloc[idx2][id_field],
                'comment1_content': df.iloc[idx1]['content'],
                'comment2_content': df.iloc[idx2]['content'],
                'similarity': similarity,
                'time_diff': time_diff,
                'is_imitative': (similarity > self.similarity_threshold) and (time_diff < self.time_diff_threshold)
            })
        
        return pd.DataFrame(results)
    
    def _identify_imitative_comments(self, df: pd.DataFrame, similarity_results: pd.DataFrame) -> pd.DataFrame:
        """è¯†åˆ«æ¨¡ä»¿æ€§è¯„è®º"""
        imitative_pairs = similarity_results[similarity_results['is_imitative'] == True]
        
        # è·å–æ¨¡ä»¿æ€§è¯„è®ºçš„è¯¦ç»†ä¿¡æ¯
        imitative_comment_ids = imitative_pairs['comment1_id'].tolist() + imitative_pairs['comment2_id'].tolist()
        # å¤„ç†ä¸åŒçš„å­—æ®µå
        id_field = 'id' if 'id' in df.columns else 'comment_id'
        imitative_comments = df[df[id_field].isin(imitative_comment_ids)].copy()
        
        # æ·»åŠ ç›¸ä¼¼åº¦ä¿¡æ¯ï¼ˆç®€åŒ–å¤„ç†ï¼‰
        imitative_comments['similarity'] = 0.0
        imitative_comments['time_diff'] = 0.0
        
        return imitative_comments
    
    def _generate_statistics(self, df: pd.DataFrame, similarity_results: pd.DataFrame, 
                           imitative_comments: pd.DataFrame) -> Dict:
        """ç”Ÿæˆç»Ÿè®¡ç»“æœ"""
        total_comments = len(df)
        total_pairs = len(similarity_results)
        imitative_pairs = len(similarity_results[similarity_results['is_imitative'] == True])
        imitative_comments_count = len(imitative_comments)
        
        # ç›¸ä¼¼åº¦ç»Ÿè®¡
        similarities = similarity_results['similarity'].tolist()
        avg_similarity = np.mean(similarities) if similarities else 0
        median_similarity = np.median(similarities) if similarities else 0
        
        # æ—¶é—´å·®ç»Ÿè®¡
        time_diffs = similarity_results['time_diff'].tolist()
        avg_time_diff = np.mean(time_diffs) if time_diffs else 0
        
        stats = {
            'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            'module_name': 'similarity',
            'total_comments': total_comments,
            'total_pairs': total_pairs,
            'imitative_pairs': imitative_pairs,
            'imitative_comments': imitative_comments_count,
            'imitative_ratio': imitative_pairs / total_pairs if total_pairs > 0 else 0,
            'avg_similarity': float(avg_similarity),
            'median_similarity': float(median_similarity),
            'avg_time_diff': float(avg_time_diff),
            'similarity_threshold': self.similarity_threshold,
            'time_diff_threshold': self.time_diff_threshold
        }
        
        return stats
    
    def save_results(self, df: pd.DataFrame, output_dir: Optional[str] = None):
        """ä¿å­˜åˆ†æç»“æœ"""
        if output_dir is None:
            output_dir = os.path.join(PROJECT_ROOT, 'data', 'results')
        
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜ç›¸ä¼¼åº¦åˆ†æç»“æœ
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # ç›¸ä¼¼åº¦å¯¹ç»“æœ
        similarity_file = os.path.join(output_dir, f'similarity_analysis_{timestamp}.csv')
        self.similarity_results.to_csv(similarity_file, index=False, encoding='utf-8')
        print(f"âœ… ç›¸ä¼¼åº¦åˆ†æç»“æœå·²ä¿å­˜åˆ°: {similarity_file}")
        
        # æ¨¡ä»¿æ€§è¯„è®ºç»“æœ
        imitative_file = os.path.join(output_dir, f'imitative_comments_{timestamp}.csv')
        self.imitative_comments.to_csv(imitative_file, index=False, encoding='utf-8')
        print(f"âœ… æ¨¡ä»¿æ€§è¯„è®ºç»“æœå·²ä¿å­˜åˆ°: {imitative_file}")
        
        # JSONæ ¼å¼ç»“æœ
        json_file = os.path.join(output_dir, f'similarity_analysis_{timestamp}.json')
        json_data = {
            'similarity_pairs': self.similarity_results.to_dict('records'),
            'imitative_comments': self.imitative_comments.to_dict('records')
        }
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSONç»“æœå·²ä¿å­˜åˆ°: {json_file}")
    
    def generate_report(self, df: pd.DataFrame):
        """ç”Ÿæˆåˆ†ææŠ¥å‘Š"""
        if not hasattr(self, 'similarity_results'):
            print("âŒ è¯·å…ˆæ‰§è¡Œåˆ†æ")
            return
        
        report_dir = os.path.join(PROJECT_ROOT, 'data', 'reports')
        os.makedirs(report_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        report_file = os.path.join(report_dir, f'similarity_analysis_report_{timestamp}.json')
        
        # ç”ŸæˆæŠ¥å‘Šæ•°æ®
        stats = self._generate_statistics(df, self.similarity_results, self.imitative_comments)
        
        # æ·»åŠ è¯¦ç»†ç»Ÿè®¡
        similarity_stats = {
            'similarity_distribution': {
                '0.0-0.2': len(self.similarity_results[self.similarity_results['similarity'] < 0.2]),
                '0.2-0.4': len(self.similarity_results[(self.similarity_results['similarity'] >= 0.2) & (self.similarity_results['similarity'] < 0.4)]),
                '0.4-0.6': len(self.similarity_results[(self.similarity_results['similarity'] >= 0.4) & (self.similarity_results['similarity'] < 0.6)]),
                '0.6-0.8': len(self.similarity_results[(self.similarity_results['similarity'] >= 0.6) & (self.similarity_results['similarity'] < 0.8)]),
                '0.8-1.0': len(self.similarity_results[self.similarity_results['similarity'] >= 0.8])
            },
            'time_diff_distribution': {
                '0-1h': len(self.similarity_results[self.similarity_results['time_diff'] < 3600]),
                '1-6h': len(self.similarity_results[(self.similarity_results['time_diff'] >= 3600) & (self.similarity_results['time_diff'] < 21600)]),
                '6-24h': len(self.similarity_results[(self.similarity_results['time_diff'] >= 21600) & (self.similarity_results['time_diff'] < 86400)]),
                '>24h': len(self.similarity_results[self.similarity_results['time_diff'] >= 86400])
            }
        }
        
        report_data = {
            **stats,
            'detailed_stats': similarity_stats,
            'analysis_parameters': {
                'similarity_threshold': self.similarity_threshold,
                'time_diff_threshold': self.time_diff_threshold,
                'min_text_length': self.min_text_length
            }
        }
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        # æ‰“å°æŠ¥å‘Šæ‘˜è¦
        print("\n=== åˆ†ææŠ¥å‘Šæ‘˜è¦ ===")
        print(f"åˆ†ææ—¶é—´: {stats['analysis_time']}")
        print(f"æ¨¡å—åç§°: {stats['module_name']}")
        print(f"æ€»è¯„è®ºæ•°: {stats['total_comments']:,}")
        print(f"çˆ¶å­è¯„è®ºå¯¹: {stats['total_pairs']:,}")
        print(f"æ¨¡ä»¿æ€§è¯„è®ºå¯¹: {stats['imitative_pairs']:,}")
        print(f"æ¨¡ä»¿æ€§è¯„è®º: {stats['imitative_comments']:,}")
        print(f"æ¨¡ä»¿æ¯”ä¾‹: {stats['imitative_ratio']:.2%}")
        print(f"å¹³å‡ç›¸ä¼¼åº¦: {stats['avg_similarity']:.3f}")
        print(f"ä¸­ä½æ•°ç›¸ä¼¼åº¦: {stats['median_similarity']:.3f}")
        print(f"å¹³å‡æ—¶é—´å·®: {stats['avg_time_diff']:.0f}ç§’")
    
    def create_visualizations(self, df: pd.DataFrame):
        """åˆ›å»ºå¯è§†åŒ–å›¾è¡¨"""
        if not hasattr(self, 'similarity_results'):
            print("âŒ è¯·å…ˆæ‰§è¡Œåˆ†æ")
            return
        
        viz_dir = os.path.join(PROJECT_ROOT, 'data', 'visualizations')
        os.makedirs(viz_dir, exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå¯è§†åŒ–', fontsize=16, fontweight='bold')
        
        # 1. ç›¸ä¼¼åº¦åˆ†å¸ƒç›´æ–¹å›¾
        ax1 = axes[0, 0]
        ax1.hist(self.similarity_results['similarity'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
        ax1.axvline(self.similarity_threshold, color='red', linestyle='--', label=f'é˜ˆå€¼: {self.similarity_threshold}')
        ax1.set_xlabel('ç›¸ä¼¼åº¦')
        ax1.set_ylabel('é¢‘æ¬¡')
        ax1.set_title('ç›¸ä¼¼åº¦åˆ†å¸ƒ')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. æ—¶é—´å·®åˆ†å¸ƒç›´æ–¹å›¾
        ax2 = axes[0, 1]
        ax2.hist(self.similarity_results['time_diff'] / 3600, bins=20, alpha=0.7, color='lightgreen', edgecolor='black')
        ax2.axvline(self.time_diff_threshold / 3600, color='red', linestyle='--', label=f'é˜ˆå€¼: {self.time_diff_threshold/3600:.1f}h')
        ax2.set_xlabel('æ—¶é—´å·®(å°æ—¶)')
        ax2.set_ylabel('é¢‘æ¬¡')
        ax2.set_title('æ—¶é—´å·®åˆ†å¸ƒ')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. ç›¸ä¼¼åº¦vsæ—¶é—´å·®æ•£ç‚¹å›¾
        ax3 = axes[1, 0]
        scatter = ax3.scatter(self.similarity_results['similarity'], 
                            self.similarity_results['time_diff'] / 3600,
                            c=self.similarity_results['is_imitative'], 
                            cmap='RdYlGn', alpha=0.6)
        ax3.axhline(self.time_diff_threshold / 3600, color='red', linestyle='--', alpha=0.5)
        ax3.axvline(self.similarity_threshold, color='red', linestyle='--', alpha=0.5)
        ax3.set_xlabel('ç›¸ä¼¼åº¦')
        ax3.set_ylabel('æ—¶é—´å·®(å°æ—¶)')
        ax3.set_title('ç›¸ä¼¼åº¦ vs æ—¶é—´å·®')
        ax3.grid(True, alpha=0.3)
        
        # 4. æ¨¡ä»¿æ€§è¯„è®ºç»Ÿè®¡
        ax4 = axes[1, 1]
        labels = ['éæ¨¡ä»¿æ€§', 'æ¨¡ä»¿æ€§']
        sizes = [len(self.similarity_results[~self.similarity_results['is_imitative']]),
                 len(self.similarity_results[self.similarity_results['is_imitative']])]
        if sum(sizes) > 0:
            colors = ['lightcoral', 'lightblue']
            ax4.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)
            ax4.set_title('æ¨¡ä»¿æ€§è¯„è®ºæ¯”ä¾‹')
        else:
            ax4.text(0.5, 0.5, 'æ— æ¨¡ä»¿æ€§ç»Ÿè®¡æ•°æ®', ha='center', va='center', transform=ax4.transAxes, fontsize=14)
            ax4.set_xlim(0, 1)
            ax4.set_ylim(0, 1)
            ax4.axis('off')
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        viz_file = os.path.join(viz_dir, f'similarity_analysis_main_{timestamp}.png')
        plt.savefig(viz_file, dpi=300, bbox_inches='tight')
        print(f"âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜åˆ°: {viz_file}")
        plt.close()


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå·¥å…·')
    parser.add_argument('--video-id', type=str, help='è§†é¢‘IDï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ†ææ‰€æœ‰æ•°æ®')
    parser.add_argument('--limit', type=int, help='é™åˆ¶åˆ†ææ•°é‡')
    parser.add_argument('--use-cleaned-data', action='store_true', help='ä½¿ç”¨æ¸…æ´—åçš„æ•°æ®æ–‡ä»¶')
    parser.add_argument('--cleaned-data-path', type=str, help='æ¸…æ´—æ•°æ®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--similarity-threshold', type=float, default=0.7, help='ç›¸ä¼¼åº¦é˜ˆå€¼')
    parser.add_argument('--time-diff-threshold', type=int, default=3600, help='æ—¶é—´å·®é˜ˆå€¼(ç§’)')
    parser.add_argument('--test', action='store_true', help='æµ‹è¯•æ¨¡å¼ï¼Œåªåˆ†æå°‘é‡æ•°æ®')
    parser.add_argument('--no-save', action='store_true', help='ä¸ä¿å­˜ç»“æœæ–‡ä»¶')
    parser.add_argument('--no-report', action='store_true', help='ä¸ç”Ÿæˆåˆ†ææŠ¥å‘Š')
    parser.add_argument('--no-viz', action='store_true', help='ä¸åˆ›å»ºå¯è§†åŒ–å›¾è¡¨')
    # å‘é‡è¯·æ±‚æ€§èƒ½å‚æ•°
    parser.add_argument('--vector-batch-size', type=int, default=100, help='å‘é‡APIæ‰¹å¤§å°ï¼Œé»˜è®¤100')
    parser.add_argument('--vector-concurrency', type=int, default=8, help='å‘é‡APIå¹¶å‘æ•°ï¼Œé»˜è®¤8')
    parser.add_argument('--vector-throttle-ms', type=int, default=0, help='å‘é‡APIèŠ‚æµæ¯«ç§’ï¼Œé»˜è®¤0=ä¸é™åˆ¶')
    
    args = parser.parse_args()
    
    # æµ‹è¯•æ¨¡å¼è®¾ç½®
    if args.test and not args.limit:
        args.limit = 10
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼ï¼šåªåˆ†æå°‘é‡æ•°æ®")
    
    # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
    print(f"=== æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå·¥å…· ===")
    if args.video_id:
        print(f"è§†é¢‘ID: {args.video_id}")
    if args.limit:
        print(f"é™åˆ¶æ•°é‡: {args.limit}")
    if args.use_cleaned_data:
        print("ä½¿ç”¨æ¸…æ´—æ•°æ®")
    print(f"ç›¸ä¼¼åº¦é˜ˆå€¼: {args.similarity_threshold}")
    print(f"æ—¶é—´å·®é˜ˆå€¼: {args.time_diff_threshold}ç§’")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åˆ†æå™¨
        analyzer = SimilarityAnalyzer(
            similarity_threshold=args.similarity_threshold,
            time_diff_threshold=args.time_diff_threshold,
            vector_batch_size=max(1, args.vector_batch_size),
            vector_concurrency=max(1, args.vector_concurrency),
            vector_throttle_ms=max(0, args.vector_throttle_ms),
        )
        
        # åŠ è½½æ•°æ®
        if args.use_cleaned_data:
            # ä»æ¸…æ´—æ•°æ®æ–‡ä»¶åŠ è½½
            if args.cleaned_data_path:
                cleaned_data_path = args.cleaned_data_path
            else:
                auto_path = resolve_latest_cleaned_data(args.video_id)
                cleaned_data_path = auto_path or os.path.join(PROJECT_ROOT, 'data', 'processed', 'douyin_comments_processed.json')
            
            if not os.path.exists(cleaned_data_path):
                print(f"âŒ æ¸…æ´—æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {cleaned_data_path}")
                return
            
            with open(cleaned_data_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            df = pd.DataFrame(data)
            print(f"âœ… æˆåŠŸåŠ è½½æ¸…æ´—æ•°æ®: {len(df)} æ¡è®°å½•")
            
            # é™åˆ¶æ•°æ®é‡
            if args.limit and len(df) > args.limit:
                df = df.head(args.limit)
                print(f"âœ… é™åˆ¶æ•°æ®é‡: {len(df)} æ¡è®°å½•")
        else:
            # ä»æ•°æ®åº“åŠ è½½
            df = analyzer._load_from_database(args.video_id, args.limit)
            if df.empty:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®")
                return
        
        # æ‰§è¡Œåˆ†æ
        stats = analyzer.analyze(df)
        
        if 'error' in stats:
            print(f"âŒ åˆ†æå¤±è´¥: {stats['error']}")
            return
        
        # ä¿å­˜ç»“æœ
        if not args.no_save:
            analyzer.save_results(df)
        
        # ç”ŸæˆæŠ¥å‘Š
        if not args.no_report:
            analyzer.generate_report(df)
        
        # åˆ›å»ºå¯è§†åŒ–
        if not args.no_viz:
            analyzer.create_visualizations(df)
        
        print("\nâœ… æ–‡æœ¬ç›¸ä¼¼åº¦åˆ†æå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ åˆ†æè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
