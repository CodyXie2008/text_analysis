#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ•°æ®é¢„å¤„ç†æ¨¡å—
æ ¹æ®æ•°æ®é¢„å¤„ç†æ–‡æ¡£å®ç°æ ‡å‡†åŒ–æ¸…æ´—ä¸ç»“æ„åŒ–å¤„ç†
"""

import os
import sys
import re
import json
import yaml
import argparse
from datetime import datetime
from typing import Dict, List, Union, Optional, Set
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

import pandas as pd
import jieba

from core.base_analyzer import BaseAnalyzer, create_parser, parse_common_args
from core.db_config import get_db_conn
from core.data_paths import AnalysisPathManager, PROJECT_ROOT, ensure_directories

class DataPreprocessingAnalyzer(BaseAnalyzer):
    """æ•°æ®é¢„å¤„ç†åˆ†æå™¨"""
    
    def __init__(self, video_id: str = None):
        super().__init__("preprocessing", video_id)
        self.config = self._load_config()
        self.stop_words = self._load_stop_words()
        self._load_user_dict()
        # ç¡®ä¿æ‰€æœ‰ç›®å½•å­˜åœ¨
        ensure_directories()
        # ä½¿ç”¨é»˜è®¤è¾“å‡ºç›®å½•
        self.processed_dir = PROJECT_ROOT / 'data' / 'processed'
        os.makedirs(self.processed_dir, exist_ok=True)
    
    def _load_config(self) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config = {
            'text': {
                'min_length': 2,
                'stopwords_path': 'hit_stopwords.txt',
                'jieba_dict': 'userdict.txt'
            },
            'filter': {
                'stopwords': ['å…³æ³¨æˆ‘', 'ç‚¹å‡»', 'é“¾æ¥', 'å¹¿å‘Š', 'ä»£è´­', 'åŠ å¾®ä¿¡', 'åŠ qq']
            },
            'output': {
                'json_path': './processed_json'
            }
        }
        
        # å°è¯•åŠ è½½é…ç½®æ–‡ä»¶
        config_path = PROJECT_ROOT / 'config.yaml'
        if os.path.exists(config_path):
            try:
                with open(config_path, 'r', encoding='utf-8') as f:
                    loaded_config = yaml.safe_load(f)
                    if loaded_config:
                        # æ›´æ–°é…ç½®
                        for section, values in loaded_config.items():
                            if section in config and isinstance(values, dict):
                                config[section].update(values)
                        print(f"âœ… æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            except Exception as e:
                print(f"âš ï¸  åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        else:
            print("âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        
        return config
    
    def _load_user_dict(self):
        """åŠ è½½ç”¨æˆ·è¯å…¸"""
        try:
            dict_path = self.config['text']['jieba_dict']
            if not os.path.isabs(dict_path):
                # å°è¯•åœ¨modulesç›®å½•ä¸‹æŸ¥æ‰¾
                module_dir = os.path.dirname(os.path.abspath(__file__))
                dict_path = os.path.join(module_dir, dict_path)
            
            if os.path.exists(dict_path):
                jieba.load_userdict(dict_path)
                print(f"âœ… æˆåŠŸåŠ è½½ç”¨æˆ·è¯å…¸: {dict_path}")
            else:
                print("âš ï¸  ç”¨æˆ·è¯å…¸æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤è¯å…¸")
        except Exception as e:
            print(f"âš ï¸  åŠ è½½ç”¨æˆ·è¯å…¸å¤±è´¥: {e}")
    
    def _load_stop_words(self):
        """åŠ è½½åœç”¨è¯"""
        stop_words = set()
        try:
            # ä»é…ç½®ä¸­è·å–åœç”¨è¯è·¯å¾„
            stopwords_path = self.config['text']['stopwords_path']
            if not os.path.isabs(stopwords_path):
                # å°è¯•åœ¨modulesç›®å½•ä¸‹æŸ¥æ‰¾
                module_dir = os.path.dirname(os.path.abspath(__file__))
                stop_words_file = os.path.join(module_dir, stopwords_path)
            else:
                stop_words_file = stopwords_path
            
            if os.path.exists(stop_words_file):
                with open(stop_words_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        # è·³è¿‡æ³¨é‡Šè¡Œå’Œç©ºè¡Œ
                        if line.strip().startswith('#') or not line.strip():
                            continue
                        word = line.strip()
                        # åªä¿ç•™æœ‰æ•ˆçš„ä¸­æ–‡ã€è‹±æ–‡åœç”¨è¯
                        if word and (re.search(r'[\u4e00-\u9fa5]', word) or re.search(r'[a-zA-Z]', word)):
                            stop_words.add(word)
                print(f"âœ… æˆåŠŸåŠ è½½ {len(stop_words)} ä¸ªåœç”¨è¯")
            else:
                # ä½¿ç”¨åŸºç¡€åœç”¨è¯
                stop_words = {
                    'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 
                    'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 
                    'è‡ªå·±', 'è¿™', 'é‚£', 'ä»–', 'å¥¹', 'å®ƒ', 'ä»¬', 'ä¸ª', 'åª', 'æ¡', 'å¼ ', 'ç‰‡'
                }
                print(f"âš ï¸ ä½¿ç”¨é»˜è®¤åœç”¨è¯é›†")
                
        except Exception as e:
            print(f"åŠ è½½åœç”¨è¯å¤±è´¥: {e}")
            # ç¡®ä¿å³ä½¿åŠ è½½å¤±è´¥ä¹Ÿæœ‰åŸºæœ¬åœç”¨è¯å¯ç”¨
            stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€'}
        
        # æ·»åŠ é…ç½®æ–‡ä»¶ä¸­çš„è¿‡æ»¤è¯
        if 'stopwords' in self.config.get('filter', {}):
            filter_stopwords = self.config['filter']['stopwords']
            stop_words.update(filter_stopwords)
            print(f"âœ… ä»é…ç½®ä¸­æ·»åŠ  {len(filter_stopwords)} ä¸ªè¿‡æ»¤è¯")
            
        return stop_words
    
    def _load_from_database(self, limit: Optional[int] = None) -> pd.DataFrame:
        """ä»æ•°æ®åº“åŠ è½½æ•°æ®"""
        print("=== ä»æ•°æ®åº“åŠ è½½è¯„è®ºæ•°æ® ===")
        
        # ä½¿ç”¨pymysqlç›´æ¥æŸ¥è¯¢ï¼Œä¸é€šè¿‡pandas
        sql = "SELECT * FROM douyin_aweme_comment WHERE content IS NOT NULL"
        
        params = []
        
        if self.video_id:
            sql += " AND aweme_id = %s"
            params = [self.video_id]
            print(f"åŠ è½½è§†é¢‘ {self.video_id} çš„è¯„è®º...")
        else:
            print("åŠ è½½æ‰€æœ‰è¯„è®º...")
        
        # å…ˆæ·»åŠ ORDER BYï¼Œå†æ·»åŠ LIMIT
        sql += " ORDER BY create_time DESC"
        
        if limit:
            sql += f" LIMIT {limit}"
            print(f"é™åˆ¶åŠ è½½æ•°é‡: {limit}")
        
        try:
            # ç›´æ¥ä½¿ç”¨conn.cursor()è·å–æ¸¸æ ‡
            with self.conn.cursor() as cursor:
                print(f"æ‰§è¡ŒSQL: {sql}")
                cursor.execute(sql, params)
                rows = cursor.fetchall()
                
                print(f"ç›´æ¥ä»æ•°æ®åº“è·å–åˆ° {len(rows)} æ¡è®°å½•")
                
                # æ‰“å°å‰2æ¡åŸå§‹æ•°æ®ç”¨äºè°ƒè¯•
                if rows:
                    print("å‰2æ¡åŸå§‹æ•°æ®:")
                    for i, row in enumerate(rows[:2]):
                        print(f"  è®°å½•{i+1}: {row}")
                        # æ£€æŸ¥contentå­—æ®µæ˜¯å¦ä¸ºå®é™…å†…å®¹
                        if isinstance(row, dict):
                            content = row.get('content', 'æ— å†…å®¹')
                            print(f"    contentå­—æ®µ: {content}")
                        else:
                            print(f"    è®°å½•ç±»å‹: {type(row)}")
                
                # å°†è·å–çš„æ•°æ®è½¬æ¢ä¸ºDataFrame
                if rows:
                    # ç¡®ä¿rowsä¸­çš„å…ƒç´ éƒ½æ˜¯å­—å…¸
                    if isinstance(rows[0], dict):
                        df = pd.DataFrame(rows)
                    else:
                        # å¦‚æœæ˜¯å…ƒç»„ï¼Œéœ€è¦è·å–åˆ—å
                        if cursor.description:
                            columns = [desc[0] for desc in cursor.description]
                            df = pd.DataFrame(rows, columns=columns)
                        else:
                            # æ— æ³•è·å–åˆ—åï¼Œä½¿ç”¨é»˜è®¤åˆ—å
                            df = pd.DataFrame(rows)
                    
                    # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
                    required_columns = ['comment_id', 'parent_comment_id', 'content', 'aweme_id', 'create_time']
                    
                    # å¤„ç†å¯èƒ½çš„åˆ—åå·®å¼‚
                    column_mapping = {
                        'parent_comment_id': ['parent_comment_id', 'parent_id'],
                        'user_id': ['user_id', 'userid'],
                        'nickname': ['nickname', 'user_nickname'],
                        'user_signature': ['user_signature', 'signature'],
                        'ip_location': ['ip_location', 'location']
                    }
                    
                    # é‡å‘½ååˆ—
                    for target_col, source_cols in column_mapping.items():
                        for source_col in source_cols:
                            if source_col in df.columns and target_col not in df.columns:
                                df = df.rename(columns={source_col: target_col})
                                print(f"âš ï¸  å°†åˆ— '{source_col}' é‡å‘½åä¸º '{target_col}'")
                                break
                    
                    # ç¡®ä¿æ‰€æœ‰å¿…è¦åˆ—éƒ½å­˜åœ¨
                    for col in required_columns:
                        if col not in df.columns:
                            print(f"âš ï¸  ç¼ºå°‘åˆ— '{col}'ï¼Œåˆ›å»ºç©ºåˆ—")
                            df[col] = ''
                    
                    # æ•°æ®ç±»å‹è½¬æ¢
                    numeric_columns = ['like_count', 'sub_comment_count']
                    for col in numeric_columns:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype(int)
                    
                    print(f"âœ… æˆåŠŸåŠ è½½å¹¶è½¬æ¢ {len(df)} æ¡è¯„è®ºåˆ°DataFrame")
                    # æ‰“å°å‰2æ¡æ•°æ®ç”¨äºè°ƒè¯•
                    if len(df) > 0:
                        print("å‰2æ¡æ•°æ®æ ·æœ¬:")
                        for i in range(min(2, len(df))):
                            content = str(df.iloc[i]['content']) if pd.notna(df.iloc[i]['content']) else 'æ— å†…å®¹'
                            print(f"  è®°å½•{i+1}: è¯„è®ºID={df.iloc[i]['comment_id']}, å†…å®¹é¢„è§ˆ={content[:30]}...")
                    return df
                else:
                    # è¿”å›ç©ºçš„DataFrame
                    print("âš ï¸  æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œè¿”å›ç©ºDataFrame")
                    return pd.DataFrame(columns=['comment_id', 'parent_comment_id', 'content', 'aweme_id', 'create_time'])
        except Exception as e:
            print(f"âŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            raise e
    
    def clean_text(self, text: str) -> str:
        """æ–‡æœ¬æ¸…æ´—"""
        if not text:
            return ""
        
        # 1. å»é™¤HTMLæ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)
        
        # 2. å»é™¤URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        
        # 3. ç§»é™¤@ç”¨æˆ·ï¼ˆæ ¼å¼ï¼š@ç”¨æˆ·å æˆ– @ç”¨æˆ·åç©ºæ ¼ï¼‰
        text = re.sub(r'@[^\s]+', '', text)
        
        # 4. å»é™¤æ–¹æ‹¬å·å†…å®¹ï¼ˆå¦‚[æ³ªå¥”][666][æ¯”å¿ƒ]ç­‰è¡¨æƒ…ï¼‰
        text = re.sub(r'\[[^\]]*\]', '', text)
        
        # 5. å»é™¤å¤šä½™ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text).strip()
        
        # 6. ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—ã€ä¸­æ–‡æ ‡ç‚¹ï¼Œå»é™¤å…¶ä»–ç¬¦å·
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\u3000-\u303f\uff00-\uffef]', '', text)
        
        return text
    
    def segment_text(self, text: str) -> List[str]:
        """æ–‡æœ¬åˆ†è¯ï¼ˆä¼˜åŒ–ç‰ˆï¼‰"""
        if not text:
            return []
        
        # ä½¿ç”¨jiebaåˆ†è¯ï¼Œå¯ç”¨HMMæ¨¡å‹æé«˜æœªç™»å½•è¯è¯†åˆ«ç‡
        words = jieba.lcut(text, HMM=True)
        
        # è¿‡æ»¤åœç”¨è¯å’Œä½è´¨é‡è¯æ±‡
        filtered_words = []
        # å®šä¹‰å¸¸è§çš„æ— æ„ä¹‰å•å­—ç¬¦ï¼ˆé™¤äº†æ•°å­—å’Œé‡è¦çš„é‡è¯ï¼‰
        meaningless_chars = {'çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€ä¸ª', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¯´', 'è¦', 'å»', 'ä½ ', 'ä¼š', 'ç€', 'æ²¡æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'è¿™'}
        
        # ä¸´æ—¶å­˜å‚¨è¿ç»­æ•°å­—å’Œè‹±æ–‡
        temp_num_eng = ""
        
        for word in words:
            word = word.strip()
            if not word:
                continue
            
            # å¤„ç†è¿ç»­çš„æ•°å­—å’Œè‹±æ–‡
            if re.match(r'^[0-9a-zA-Z]+$', word):
                temp_num_eng += word
                continue
            else:
                # å¦‚æœä¸´æ—¶å­˜å‚¨ä¸ä¸ºç©ºï¼Œå…ˆå¤„ç†å®ƒ
                if temp_num_eng:
                    if len(temp_num_eng) > 1:  # æ•°å­—å’Œè‹±æ–‡ç»„åˆä¿ç•™
                        filtered_words.append(temp_num_eng)
                    temp_num_eng = ""
            
            # è¿‡æ»¤æ¡ä»¶ï¼š
            # 1. ä¸æ˜¯åœç”¨è¯
            # 2. ä¸æ˜¯æ— æ„ä¹‰å•å­—ç¬¦
            # 3. é•¿åº¦å¤§äº1 æˆ–è€… æ˜¯é‡è¦çš„é‡è¯/æ•°å­—
            if (word not in self.stop_words and 
                word not in meaningless_chars and 
                (len(word) > 1 or (len(word) == 1 and word.isdigit()))):
                filtered_words.append(word)
        
        # å¤„ç†æœ€åå‰©ä½™çš„æ•°å­—å’Œè‹±æ–‡
        if temp_num_eng and len(temp_num_eng) > 1:
            filtered_words.append(temp_num_eng)
        
        # åˆ†è¯åå¤„ç†ï¼šåˆå¹¶é‡å¤çš„è¯ï¼ˆå¦‚"å“ˆå“ˆå“ˆå“ˆ" -> "å“ˆå“ˆ"ï¼‰
        if len(filtered_words) > 0:
            processed_words = [filtered_words[0]]
            for word in filtered_words[1:]:
                # æ£€æŸ¥æ˜¯å¦ä¸å‰ä¸€ä¸ªè¯é‡å¤ä¸”éƒ½æ˜¯ç®€å•è¯
                if word != processed_words[-1] or len(word) > 2:
                    processed_words.append(word)
            return processed_words
        
        return filtered_words
    
    def build_comment_tree(self, df: pd.DataFrame) -> Dict:
        """æ„å»ºè¯„è®ºæ ‘ç»“æ„ï¼Œåªä¿ç•™å»ºæ¨¡æ‰€éœ€çš„å¿…è¦å­—æ®µï¼Œä¼˜åŒ–ç»“æ„ä¾¿äºç”µè„‘è°ƒç”¨å’Œäººå·¥è§‚å¯Ÿ
        
        ä¼˜åŒ–ç‚¹ï¼š
        1. sub_comment_countä»…åœ¨çˆ¶è¯„è®ºä¸­æ˜¾ç¤º
        2. å°†sub_comment_countæ”¾åœ¨childrenå‰é¢ï¼Œä½¿ç»“æ„æ›´æ¸…æ™°
        3. ä¼˜åŒ–å­—æ®µé¡ºåºï¼Œä½¿æ ¸å¿ƒä¿¡æ¯åœ¨å‰é¢å±•ç¤º
        """
        print("=== æ„å»ºè¯„è®ºæ ‘ ===")
        
        # ä¸ºæ¯ä¸ªè¯„è®ºåˆ›å»ºå­—å…¸ï¼Œåªä¿ç•™å»ºæ¨¡æ‰€éœ€çš„å¿…è¦å­—æ®µ
        comment_dict = {}
        for idx, row in df.iterrows():
            # å®‰å…¨åœ°å¤„ç†æ¯ä¸€è¡Œæ•°æ®
            try:
                row_dict = row.to_dict()
                # ç¡®ä¿comment_idå­˜åœ¨ä¸”ä¸ºå­—ç¬¦ä¸²
                comment_id = str(row_dict.get('comment_id', f'comment_{idx}'))
                parent_id = str(row_dict.get('parent_comment_id', ''))
                
                # ä¼˜åŒ–å­—æ®µé¡ºåºï¼Œå°†æ ¸å¿ƒä¿¡æ¯æ”¾åœ¨å‰é¢
                # ä¼˜å…ˆä½¿ç”¨æ¸…æ´—åçš„content_cleanedå­—æ®µï¼Œå¦‚æœä¸å­˜åœ¨åˆ™ä½¿ç”¨åŸå§‹content
                content_value = row_dict.get('content_cleaned', row_dict.get('content', ''))
                essential_comment = {
                    'comment_id': comment_id,
                    'parent_comment_id': parent_id,
                    'content': content_value,
                    'datetime': row_dict.get('datetime', datetime.now().strftime('%Y-%m-%d %H:%M:%S')),
                    'like_count': int(row_dict.get('like_count', 0)) if pd.notna(row_dict.get('like_count')) else 0,
                    'user_id': str(row_dict.get('user_id', '')),
                    'nickname': row_dict.get('nickname', ''),
                    'ip_location': row_dict.get('ip_location', ''),
                    'tokens': row_dict.get('tokens', []),
                    'children': []
                    # sub_comment_countæš‚æ—¶ä¸åœ¨è¿™é‡Œæ·»åŠ ï¼Œåªåœ¨éœ€è¦æ—¶æ·»åŠ åˆ°çˆ¶è¯„è®º
                }
                
                comment_dict[comment_id] = essential_comment
            except Exception as e:
                print(f"   - å¤„ç†è¡Œ {idx} æ—¶å‡ºé”™: {e}")
                continue
        
        # æ„å»ºè¯„è®ºæ ‘
        root_comments = []
        parent_found = set()
        
        # ç¬¬ä¸€æ¬¡éå†ï¼šå¤„ç†IDæ ¼å¼ï¼Œç¡®ä¿ä¸€è‡´æ€§
        for comment_id in list(comment_dict.keys()):
            comment = comment_dict[comment_id]
            # ç¡®ä¿æ‰€æœ‰IDéƒ½æ˜¯å­—ç¬¦ä¸²
            comment_id_str = str(comment.get('comment_id', comment_id))
            parent_id_str = comment.get('parent_comment_id')
            
            # è½¬æ¢parent_idä¸ºå­—ç¬¦ä¸²è¿›è¡Œæ¯”è¾ƒ
            if pd.isna(parent_id_str) or parent_id_str is None:
                parent_id_str = ''
            parent_id_str = str(parent_id_str)
            
            # æ›´æ–°commentä¸­çš„IDå€¼
            comment['comment_id'] = comment_id_str
            comment['parent_comment_id'] = parent_id_str
        
        # æ‰¾å‡ºæ‰€æœ‰çˆ¶è¯„è®ºIDï¼Œè¿™äº›è¯„è®ºéœ€è¦æ·»åŠ sub_comment_count
        potential_parent_ids = set()
        for comment_id in comment_dict:
            parent_id = comment_dict[comment_id].get('parent_comment_id')
            if parent_id and parent_id != '0' and parent_id in comment_dict:
                potential_parent_ids.add(parent_id)
        
        # ä¸ºæ‰€æœ‰æ½œåœ¨çš„çˆ¶è¯„è®ºæ·»åŠ sub_comment_countå­—æ®µï¼Œåˆå§‹åŒ–ä¸º0
        for parent_id in potential_parent_ids:
            # åˆ›å»ºæ–°çš„æœ‰åºå­—å…¸ï¼Œç¡®ä¿sub_comment_countåœ¨childrenå‰é¢
            original_comment = comment_dict[parent_id]
            ordered_comment = {}
            
            # æŒ‰ç…§ä¼˜åŒ–çš„é¡ºåºæ·»åŠ å­—æ®µ
            for key in ['comment_id', 'parent_comment_id', 'content', 'datetime', 
                       'like_count', 'user_id', 'nickname', 'ip_location', 'tokens']:
                if key in original_comment:
                    ordered_comment[key] = original_comment[key]
            
            # æ·»åŠ sub_comment_countï¼Œæ”¾åœ¨childrenå‰é¢
            ordered_comment['sub_comment_count'] = 0
            ordered_comment['children'] = original_comment.get('children', [])
            
            comment_dict[parent_id] = ordered_comment
        
        # ç¬¬äºŒæ¬¡éå†ï¼šæ„å»ºæ ‘ç»“æ„å¹¶æ›´æ–°å­è¯„è®ºè®¡æ•°
        for comment_id in list(comment_dict.keys()):
            comment = comment_dict[comment_id]
            parent_id = comment.get('parent_comment_id')
            
            # æ›´å®½æ¾çš„æ ¹è¯„è®ºè¯†åˆ«ï¼šç©ºã€0ã€'0'ã€'null'ã€'None'ç­‰éƒ½è§†ä¸ºæ ¹è¯„è®º
            if not parent_id or parent_id.lower() in ['0', 'null', 'none']:
                root_comments.append(comment)
            else:
                # æŸ¥æ‰¾çˆ¶è¯„è®ºå¹¶æ·»åŠ ä¸ºå­è¯„è®º
                if parent_id in comment_dict:
                    comment_dict[parent_id]['children'].append(comment)
                    # å¦‚æœçˆ¶è¯„è®ºæœ‰sub_comment_countå­—æ®µï¼Œåˆ™å¢åŠ è®¡æ•°
                    if 'sub_comment_count' in comment_dict[parent_id]:
                        comment_dict[parent_id]['sub_comment_count'] += 1
                    parent_found.add(comment_id)
        
        # å¤„ç†å­¤ç«‹çš„å­è¯„è®ºï¼ˆçˆ¶è¯„è®ºä¸å­˜åœ¨ï¼‰ï¼Œå°†å®ƒä»¬æå‡ä¸ºæ ¹è¯„è®º
        for comment_id in list(comment_dict.keys()):
            if comment_id not in parent_found and comment_id not in [c['comment_id'] for c in root_comments]:
                root_comments.append(comment_dict[comment_id])
        
        # æ„å»ºæœ€ç»ˆçš„æ ‘ç»“æ„
        tree = {'root': root_comments}
        
        parent_count = len(root_comments)
        total_comments = len(comment_dict)
        child_count = total_comments - parent_count
        
        print(f"   - æ ¹è¯„è®ºæ•°é‡: {parent_count}")
        print(f"   - å­è¯„è®ºæ•°é‡: {child_count}")
        print(f"   - è¯„è®ºæ ‘æ„å»ºå®Œæˆ")
        
        return tree
    
    def analyze(self, df: pd.DataFrame, strict_mode: bool = False) -> Dict:
        """æ‰§è¡Œæ•°æ®é¢„å¤„ç†åˆ†æ"""
        min_length = self.config['text']['min_length']  # ä»é…ç½®è·å–æœ€å°é•¿åº¦é™åˆ¶
        print(f"=== å¼€å§‹æ•°æ®é¢„å¤„ç† ===")
        print(f"   - åŸå§‹æ•°æ®æ€»æ•°: {len(df)}")
        print(f"   - æœ€å°é•¿åº¦é™åˆ¶: {min_length}")
        
        # è®°å½•åŸå§‹æ•°æ®ç»Ÿè®¡
        original_stats = {
            'total_comments': int(len(df)),
            'unique_videos': int(df['aweme_id'].nunique()),
            'unique_users': int(df['user_id'].nunique()) if 'user_id' in df.columns else 0
        }
        
        # 1. åˆ é™¤æ— æ•ˆå†…å®¹
        print("1. è¿‡æ»¤æ— æ•ˆå†…å®¹...")
        df_filtered = df[df['content'].str.strip().str.len() >= min_length].copy()
        removed_short = len(df) - len(df_filtered)
        print(f"   - è¿‡æ»¤çŸ­å†…å®¹: {removed_short} æ¡")
        
        # 2. å»é™¤åƒåœ¾ä¿¡æ¯ï¼ˆå¹¿å‘Šå…³é”®è¯å’Œæ¨¡å¼ï¼‰
        print("2. å»é™¤åƒåœ¾ä¿¡æ¯...")
        # ä»é…ç½®è·å–ç¦æ­¢è¯ï¼Œé»˜è®¤æä¾›æ›´å…¨é¢çš„å¹¿å‘Šå…³é”®è¯åˆ—è¡¨
        default_ban_words = [
            # å¸¸è§å¹¿å‘Šè¯æ±‡
            'å…³æ³¨æˆ‘', 'ç‚¹å‡»', 'é“¾æ¥', 'å¹¿å‘Š', 'ä»£è´­', 'åŠ å¾®ä¿¡', 'åŠ qq',
            'è”ç³»æ–¹å¼', 'æ‰«ä¸€æ‰«', 'äºŒç»´ç ', 'ä¼˜æƒ ', 'æŠ˜æ‰£', 'ä¿ƒé”€', 'è¿”åˆ©',
            'èµšé’±', 'å…¼èŒ', 'åˆ·å•', 'æ¨å¹¿', 'ä»£ç†', 'åŠ ç›Ÿ', 'å…è´¹é¢†å–',
            'è–‡\S*ä¿¡', 'V\S*X', 'v\S*x', 'Q\S*Q', 'q\S*q',
            # ç”µè¯å·ç æ¨¡å¼
            '1[3-9]\d{9}', 
            # ç½‘å€ç‰¹å¾
            'www\.', '.com', '.cn', '.net',
            # ç‰¹æ®Šç¬¦å·ç»„åˆï¼ˆå¸¸è§äºå¹¿å‘Šï¼‰
            'ğŸ’–', 'ğŸ‘‰', 'ğŸ‘‡', 'ğŸ”¥', 'ç¦åˆ©', 'çº¢åŒ…', 'ç°é‡‘',
            # é‡å¤å­—ç¬¦ï¼ˆåƒåœ¾å†…å®¹ç‰¹å¾ï¼‰
            '666666', '233333', 'å“ˆå“ˆå“ˆå“ˆå“ˆ{5,}',
            # å¹³å°å¼•æµè¯æ±‡
            'æŠ–éŸ³å·', 'å¿«æ‰‹å·', 'å°çº¢ä¹¦', 'å¾®åš', 'ç›´æ’­é—´', 'ç²‰ä¸ç¾¤'
        ]
        ban_words = self.config['filter'].get('stopwords', default_ban_words)
        
        # åˆ›å»ºåƒåœ¾å†…å®¹æ£€æµ‹å‡½æ•°
        def is_spam_content(content):
            if not content:
                return False
            
            # 1. å…³é”®è¯åŒ¹é…
            for ban_word in ban_words:
                if re.search(ban_word, content, re.IGNORECASE):
                    return True
            
            # 2. æ£€æµ‹ç”µè¯å·ç æ¨¡å¼
            if re.search(r'1[3-9]\d{9}', content):
                return True
            
            # 3. æ£€æµ‹ç½‘å€æ¨¡å¼
            if re.search(r'(https?://|www\.|\.com|\.cn|\.net)', content, re.IGNORECASE):
                return True
            
            # 4. æ£€æµ‹é‡å¤å­—ç¬¦è¿‡å¤šï¼ˆåƒåœ¾å†…å®¹ç‰¹å¾ï¼‰
            if re.search(r'(.)\1{5,}', content):
                return True
            
            # 5. æ£€æµ‹emojiè¡¨æƒ…è¿‡å¤šï¼ˆåƒåœ¾å†…å®¹ç‰¹å¾ï¼‰
            emoji_pattern = re.compile(r'[\u1F600-\u1F6FF\u2600-\u26FF\u2700-\u27BF]')
            emoji_count = len(emoji_pattern.findall(content))
            if emoji_count > 3:
                return True
            
            return False
        
        # åº”ç”¨åƒåœ¾å†…å®¹è¿‡æ»¤
        df_filtered = df_filtered[~df_filtered['content'].apply(is_spam_content)].copy()
        removed_spam = len(df) - removed_short - len(df_filtered)
        print(f"   - è¿‡æ»¤å¹¿å‘Šå†…å®¹: {removed_spam} æ¡")
        
        # 3. æ–‡æœ¬æ¸…æ´—
        print("3. æ–‡æœ¬æ¸…æ´—...")
        df_filtered['content_cleaned'] = df_filtered['content'].apply(self.clean_text)
        df_filtered = df_filtered[df_filtered['content_cleaned'].str.len() >= min_length].copy()
        print(f"   - æ¸…æ´—åä¿ç•™: {len(df_filtered)} æ¡")
        
        # 3. æ–‡æœ¬æ¸…æ´—
        print("3. æ–‡æœ¬æ¸…æ´—...")
        df_filtered['content_cleaned'] = df_filtered['content'].apply(self.clean_text)
        df_filtered = df_filtered[df_filtered['content_cleaned'].str.len() >= min_length].copy()
        print(f"   - æ¸…æ´—åä¿ç•™: {len(df_filtered)} æ¡")
        
        # 4. åˆ†è¯å¤„ç†
        print("4. æ–‡æœ¬åˆ†è¯...")
        df_filtered['tokens'] = df_filtered['content_cleaned'].apply(self.segment_text)
        # è¿‡æ»¤åˆ†è¯åä¸ºç©ºçš„å†…å®¹
        df_filtered = df_filtered[df_filtered['tokens'].apply(len) > 0].copy()
        print(f"   - åˆ†è¯åä¿ç•™: {len(df_filtered)} æ¡")
        
        # 5. æ—¶é—´æ ¼å¼åŒ–
        print("5. æ—¶é—´æ ¼å¼åŒ–...")
        if 'create_time' in df_filtered.columns:
            try:
                # æ›´å¥å£®çš„æ—¶é—´å¤„ç†æ–¹å¼
                df_filtered['datetime'] = df_filtered['create_time']
                
                # å°è¯•ä¸åŒçš„æ—¶é—´æ ¼å¼å¤„ç†
                # 1. æ£€æŸ¥æ˜¯å¦ä¸ºæ•°å­—ç±»å‹ï¼ˆå¯èƒ½æ˜¯æ—¶é—´æˆ³ï¼‰
                if pd.api.types.is_numeric_dtype(df_filtered['create_time']):
                    numeric_times = pd.to_numeric(df_filtered['create_time'], errors='coerce')
                    # å°è¯•ç§’çº§æ—¶é—´æˆ³
                    df_filtered['datetime'] = pd.to_datetime(numeric_times, unit='s', errors='ignore')
                    # å¯¹äºä»æœªè½¬æ¢æˆåŠŸçš„ï¼Œå°è¯•æ¯«ç§’çº§
                    mask = pd.to_datetime(df_filtered['datetime'], errors='coerce').isna()
                    if mask.any():
                        df_filtered.loc[mask, 'datetime'] = pd.to_datetime(numeric_times[mask], unit='ms', errors='ignore')
                else:
                    # å­—ç¬¦ä¸²æ ¼å¼æ—¶é—´
                    df_filtered['datetime'] = pd.to_datetime(df_filtered['create_time'], errors='ignore')
                
                # å¯¹äºä»æ— æ³•è½¬æ¢çš„æ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´ä½œä¸ºé»˜è®¤å€¼
                mask = pd.to_datetime(df_filtered['datetime'], errors='coerce').isna()
                if mask.any():
                    print(f"   - ä½¿ç”¨é»˜è®¤æ—¶é—´æ›¿ä»£æ— æ•ˆæ—¶é—´: {mask.sum()} æ¡")
                    df_filtered.loc[mask, 'datetime'] = datetime.now()
                
                # ç¡®ä¿æ‰€æœ‰æ—¶é—´éƒ½è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
                df_filtered['datetime'] = df_filtered['datetime'].astype(str)
                # å¯¹äºå·²æˆåŠŸè½¬æ¢ä¸ºdatetimeå¯¹è±¡çš„ï¼Œæ ¼å¼åŒ–è¾“å‡º
                mask = df_filtered['datetime'].str.contains(r'\d{4}-\d{2}-\d{2}')
                if mask.any():
                    df_filtered.loc[mask, 'datetime'] = pd.to_datetime(df_filtered.loc[mask, 'datetime']).dt.strftime('%Y-%m-%d %H:%M:%S')
                
                print(f"   - æ—¶é—´æ ¼å¼åŒ–å®Œæˆï¼Œä¿ç•™æ‰€æœ‰è®°å½•")
            except Exception as e:
                print(f"   - æ—¶é—´æ ¼å¼åŒ–å‡ºé”™: {e}")
                # å‡ºé”™æ—¶ï¼Œä¸ºæ‰€æœ‰è®°å½•è®¾ç½®é»˜è®¤æ—¶é—´
                df_filtered['datetime'] = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                print("   - ä½¿ç”¨é»˜è®¤æ—¶é—´")
        
        # 6. æŒ‰è§†é¢‘IDåˆ†ç»„å¹¶æ„å»ºè¯„è®ºæ ‘
        print("6. æŒ‰è§†é¢‘åˆ†ç»„å¹¶æ„å»ºè¯„è®ºæ ‘...")
        
        # ç¡®ä¿æœ‰è§†é¢‘IDåˆ—
        if 'aweme_id' in df_filtered.columns:
            video_id_col = 'aweme_id'
        elif 'video_id' in df_filtered.columns:
            video_id_col = 'video_id'
        else:
            # å¦‚æœæ²¡æœ‰è§†é¢‘IDåˆ—ï¼Œåˆ›å»ºä¸€ä¸ªé»˜è®¤å€¼
            df_filtered['video_id'] = 'default_video'
            video_id_col = 'video_id'
        
        video_groups = df_filtered.groupby(video_id_col)
        total_videos = len(video_groups)
        print(f"   - è§†é¢‘æ€»æ•°: {total_videos}")
        
        # 7. ä¸ºæ¯ä¸ªè§†é¢‘ä¿å­˜JSONæ–‡ä»¶
        saved_videos = 0
        for video_id, group in video_groups:
            try:
                # ç¡®ä¿è§†é¢‘IDéç©º
                if pd.isna(video_id) or video_id == '':
                    video_id = 'unknown_video'
                
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²ç±»å‹
                video_id = str(video_id)
                
                # ä½¿ç”¨build_comment_treeæ–¹æ³•æ„å»ºè¯„è®ºæ ‘
                tree = self.build_comment_tree(group)
                root_comments = tree['root']
                
                # å‡†å¤‡ä¿å­˜çš„è¯„è®ºæ ‘æ•°æ®ï¼ŒåŒ…å«æ›´å®Œæ•´çš„å…ƒæ•°æ®
                comment_tree = {
                    'video_id': video_id,
                    'total_comments': len(group),
                    'root_comments': len(root_comments),
                    'create_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                    'metadata': {
                        'processing_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
                        'data_source': 'douyin_aweme_comment'
                    },
                    'root': root_comments
                }
                
                # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼šaweme_id_YYYYMMDD_HHMMSS.json
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                output_file = os.path.join(self.processed_dir, f"{video_id}_{timestamp}.json")
                with open(output_file, 'w', encoding='utf-8') as f:
                    json.dump(comment_tree, f, ensure_ascii=False, indent=2)
                saved_videos += 1
                
            except Exception as e:
                print(f"   - å¤„ç†è§†é¢‘ {video_id} å‡ºé”™: {e}")
        
        print(f"   - æˆåŠŸä¿å­˜ {saved_videos} ä¸ªè§†é¢‘çš„è¯„è®ºæ ‘åˆ° {self.processed_dir}")
        
        # æœ€ç»ˆç»Ÿè®¡
        final_stats = {
            'total_comments': int(len(df_filtered)),
            'unique_videos': int(df_filtered[video_id_col].nunique()),
            'saved_videos': saved_videos,
            'removed_short': removed_short,
            'removed_spam': removed_spam,
            'output_dir': str(self.processed_dir)
        }
        
        return {
            'original_stats': original_stats,
            'final_stats': final_stats,
            'output_dir': str(self.processed_dir)
        }
    
    def save_results(self, df: pd.DataFrame, results: Dict):
        """ä¿å­˜å¤„ç†ç»“æœï¼ˆå·²ç§»é™¤preprocessing_stats.jsonç”Ÿæˆï¼‰"""
        # æ ¹æ®éœ€æ±‚ç§»é™¤äº†ç”Ÿæˆpreprocessing_stats.jsonæ–‡ä»¶çš„åŠŸèƒ½
        # é¢„å¤„ç†ç»“æœå·²ç»é€šè¿‡å„ä¸ªè§†é¢‘çš„JSONæ–‡ä»¶ä¿å­˜ï¼Œä¸éœ€è¦é¢å¤–çš„ç»Ÿè®¡æ–‡ä»¶
        pass

def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser("preprocessing", "æ•°æ®é¢„å¤„ç†å·¥å…·")
    # æ·»åŠ é¢„å¤„ç†ç‰¹å®šå‚æ•°
    parser.add_argument('--min-length', type=int, help='æœ€å°å†…å®¹é•¿åº¦ï¼ˆè¦†ç›–é…ç½®æ–‡ä»¶ï¼‰')
    parser.add_argument('--output-dir', type=str, help='è‡ªå®šä¹‰è¾“å‡ºç›®å½•')
    parser.add_argument('--stopwords-file', type=str, help='è‡ªå®šä¹‰åœç”¨è¯æ–‡ä»¶è·¯å¾„')
    
    args = parser.parse_args()
    
    # è§£æé€šç”¨å‚æ•°
    args = parse_common_args(parser, args)
    
    # åˆ›å»ºåˆ†æå™¨
    analyzer = DataPreprocessingAnalyzer(args.video_id)
    
    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šäº†æœ€å°é•¿åº¦ï¼Œè¦†ç›–é…ç½®
    if args.min_length is not None:
        analyzer.config['text']['min_length'] = args.min_length
        print(f"âš ï¸  ä½¿ç”¨å‘½ä»¤è¡ŒæŒ‡å®šçš„æœ€å°é•¿åº¦: {args.min_length}")
    
    # å¦‚æœå‘½ä»¤è¡Œå‚æ•°æŒ‡å®šäº†åœç”¨è¯æ–‡ä»¶ï¼Œé‡æ–°åŠ è½½
    if args.stopwords_file:
        analyzer.config['text']['stopwords_path'] = args.stopwords_file
        analyzer.stop_words = analyzer._load_stop_words()
    
    # è¿è¡Œé¢„å¤„ç†
    # ç¦ç”¨æŠ¥å‘Šå’Œå¯è§†åŒ–ï¼Œåªç”Ÿæˆå¤„ç†åçš„æ•°æ®JSON
    analyzer.run_analysis(
        limit=args.limit,
        create_visualizations=False,
        generate_report=False
    )

if __name__ == "__main__":
    main()