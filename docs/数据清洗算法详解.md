# 数据清洗算法详解

## 📋 概述

数据清洗模块 (`data_cleaning_optimized.py`) 是文本分析系统的核心预处理模块，负责将原始评论数据转换为高质量的分析数据。该模块集成了时间标准化功能，为后续的从众心理时间分析提供支持。

## 🏗️ 算法架构

```
原始数据 → 数据质量检查 → 垃圾评论过滤 → 文本清洗 → 分词处理 → 时间标准化 → 输出数据
    ↓           ↓              ↓           ↓          ↓           ↓
  351条      质量统计        过滤8条      清洗343条   分词330条   标准化330条
```

## 🔧 核心算法

### 1. 垃圾评论检测算法

#### 检测规则
```python
def is_spam_comment(self, content: str, user_signature: str = None) -> bool:
    # 1. 内容长度检查
    if len(content.strip()) < 5:
        return True
    
    # 2. 特殊字符比例检查
    special_char_ratio = len(re.findall(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', content)) / len(content)
    if special_char_ratio > 0.6:
        return True
    
    # 3. 广告关键词检查
    ad_keywords = ['加微信', '加qq', '加群', '私信', '联系我', '合作', '推广', '广告', ...]
    
    # 4. 重复字符检查
    if len(set(content)) < len(content) * 0.2:
        return True
    
    # 5. 用户签名检查
    if user_signature and (len(user_signature) > 100 or re.search(r'[0-9]{8,}', user_signature)):
        return True
```

#### 算法特点
- **多维度检测**: 从内容、格式、用户信息等多个维度判断
- **阈值可调**: 特殊字符比例、重复字符比例等参数可调整
- **关键词库**: 内置广告关键词库，可扩展

### 2. 文本清洗算法

#### 清洗步骤
```python
def clean_text(self, text: str) -> str:
    # 1. 去除HTML标签
    text = re.sub(r'<[^>]+>', '', text)
    
    # 2. 去除URL
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # 3. 去除方括号内容（如[666][比心]等）
    text = re.sub(r'\[[^\]]*\]', '', text)
    
    # 4. 去除多余空白字符
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 5. 保留中文、英文、数字、中文标点，去除其他符号
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\u3000-\u303f\uff00-\uffef]', '', text)
```

#### 算法特点
- **渐进式清洗**: 按步骤逐步清理，避免过度清洗
- **保留语义**: 保留中文标点符号，维持文本语义
- **正则优化**: 使用高效的正则表达式模式

### 3. 分词算法

#### 双模式支持
```python
def segment_text(self, text: str) -> List[str]:
    if self.segment_mode == 'api' and self.aliyun_api_manager is not None:
        # 阿里云API分词
        words = self.aliyun_api_manager.segment_text(text)
    else:
        # 本地jieba分词
        words = jieba.lcut(text)
    
    # 过滤停用词和短词
    filtered_words = [word for word in words 
                     if word and word.strip() and len(word.strip()) > 1 
                     and word not in self.stop_words]
```

#### 算法特点
- **智能回退**: API失败时自动回退到本地分词
- **停用词过滤**: 基于停用词库过滤无意义词汇
- **长度过滤**: 过滤单字符词汇，提高分词质量

### 4. 时间标准化算法

#### 核心功能
```python
def _add_time_standardization(self, df: pd.DataFrame) -> pd.DataFrame:
    # 1. 时间标准化：Unix时间戳转datetime
    df['comment_time'] = pd.to_datetime(df['create_time'], unit='s')
    
    # 2. 按时间排序
    df = df.sort_values('comment_time').reset_index(drop=True)
    
    # 3. 添加父评论标识
    parent_mask = (df['parent_comment_id'].isna() | 
                  (df['parent_comment_id'] == '0') | 
                  (df['parent_comment_id'] == 0))
    df['is_parent'] = parent_mask
    
    # 4. 计算时间差
    for idx, row in df.iterrows():
        if not row['is_parent']:
            parent_time = parent_times[row['parent_comment_id']]
            time_diff = abs((row['comment_time'] - parent_time).total_seconds())
            time_diffs.append(time_diff)
```

#### 算法特点
- **父子关系识别**: 基于 `parent_comment_id` 字段准确识别父子评论关系
- **时间差计算**: 精确计算子评论与父评论的时间间隔
- **数据完整性**: 确保时间数据的完整性和一致性

## 📊 性能指标

### 处理效率
- **数据清洗速度**: 约1000条/秒
- **分词处理速度**: 本地模式约500条/秒，API模式约100条/秒
- **内存使用**: 约10MB（处理1000条评论）

### 质量指标
- **垃圾评论过滤率**: 2.3%（351条中过滤8条）
- **文本清洗保留率**: 100%（343条全部保留）
- **分词后保留率**: 96.2%（343条中保留330条）

## 🔄 数据流转

### 输入数据格式
```json
{
  "comment_id": "7306470754056569635",
  "aweme_id": "7306437681045654834",
  "parent_comment_id": "0",
  "content": "这个视频很棒！",
  "create_time": 1701170293,
  "like_count": 100,
  "sub_comment_count": 5,
  "user_id": "123456789",
  "nickname": "用户昵称",
  "user_signature": "用户签名"
}
```

### 输出数据格式
```json
{
  "comment_id": "7306470754056569635",
  "aweme_id": "7306437681045654834",
  "parent_comment_id": "0",
  "content": "这个视频很棒！",
  "create_time": 1701170293,
  "comment_time": "2023-11-28 19:18:13",
  "is_parent": true,
  "time_diff_seconds": 0,
  "words": ["这个", "视频", "很棒"],
  "word_count": 3,
  "like_count": 100,
  "sub_comment_count": 5,
  "user_id": "123456789",
  "nickname": "用户昵称",
  "user_signature": "用户签名"
}
```

## 🎯 应用场景

### 1. 数据预处理
- 为情感分析、相似度分析等模块提供高质量数据
- 统一数据格式，确保后续分析的一致性

### 2. 时间分析支持
- 为从众心理时间分析提供标准化的时间数据
- 自动计算父子评论时间差，支持时间从众心理分析

### 3. 内容质量提升
- 过滤垃圾评论，提高分析准确性
- 标准化文本格式，改善分词效果

## 🔧 配置参数

### 垃圾评论检测参数
```python
# 内容长度阈值
MIN_CONTENT_LENGTH = 5

# 特殊字符比例阈值
MAX_SPECIAL_CHAR_RATIO = 0.6

# 重复字符比例阈值
MIN_UNIQUE_CHAR_RATIO = 0.2

# 用户签名长度阈值
MAX_SIGNATURE_LENGTH = 100
```

### 分词参数
```python
# 最小词长度
MIN_WORD_LENGTH = 2

# 停用词文件路径
STOP_WORDS_FILE = "hit_stopwords.txt"
```

## 📈 优化建议

### 1. 性能优化
- 使用批量处理减少API调用次数
- 实现分词结果缓存机制
- 优化正则表达式性能

### 2. 质量提升
- 扩展广告关键词库
- 优化停用词列表
- 增加更多垃圾评论检测规则

### 3. 功能扩展
- 支持更多语言的分词
- 增加文本去重功能
- 实现增量数据清洗

## 🔍 故障排除

### 常见问题

1. **API调用失败**
   - 检查网络连接
   - 验证API密钥配置
   - 查看API调用限制

2. **分词效果不佳**
   - 检查停用词配置
   - 调整分词参数
   - 考虑使用API分词

3. **时间标准化错误**
   - 检查时间字段格式
   - 验证父子评论关系
   - 确认数据完整性

### 调试方法
```python
# 启用详细日志
import logging
logging.basicConfig(level=logging.DEBUG)

# 检查数据质量
analyzer = DataCleaningAnalyzer()
quality_stats = analyzer._check_data_quality(df)
print(quality_stats)
```

---

*数据清洗算法详解 v1.0.0 - 2025-09-05*
