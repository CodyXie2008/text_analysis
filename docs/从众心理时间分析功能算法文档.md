# 从众心理时间分析功能算法文档

## 概述

`conformity_time_analysis.py` 是从众心理分析系统的核心时间分析模块，专门用于分析抖音评论中的从众行为时间特征。该模块通过计算子评论与父评论的时间差，识别从众行为的时间窗口，为心理学研究提供数据支持。

## 一、核心功能架构

### 1.1 模块结构
```
ConformityTimeAnalyzer
├── 数据获取层
│   ├── calculate_time_differences()      # 主入口函数
│   ├── load_from_cleaned_data()         # 从清洗数据加载
│   ├── calculate_time_differences_from_data() # 从清洗数据计算时间差
│   └── calculate_time_differences_from_db() # 从数据库直接获取
├── 分析处理层
│   ├── analyze_time_distribution()      # 时间分布分析
│   ├── calculate_window_distribution()  # 时间窗口分布
│   ├── detect_conformity_windows()      # 从众窗口检测
│   └── find_dense_periods()             # 密集时间段检测
├── 可视化层
│   └── create_visualizations()          # 图表生成
└── 报告层
    └── generate_report()                # 报告生成
```

### 1.2 时间窗口定义
```python
time_windows = {
    'immediate': (0, 5),      # 0-5分钟：立即从众
    'quick': (5, 30),         # 5-30分钟：快速从众
    'medium': (30, 120),      # 30分钟-2小时：中等从众
    'slow': (120, 1440),      # 2小时-24小时：缓慢从众
    'delayed': (1440, 10080), # 1天-1周：延迟从众
    'long_term': (10080, None) # 1周以上：长期从众
}
```

## 二、核心算法详解

### 2.1 数据获取算法

#### 2.1.1 混合数据获取策略
**算法名称**: `calculate_time_differences()`
**目的**: 提供灵活的数据获取方式，支持从清洗数据或原始数据库获取

**算法流程**:
```python
def calculate_time_differences(conn, aweme_id=None, use_cleaned_data=False, cleaned_data_path=None):
    if use_cleaned_data and cleaned_data_path:
        # 策略1: 从清洗数据调取
        df = load_from_cleaned_data(cleaned_data_path)
        if df is None or df.empty:
            # 降级策略: 切换到数据库获取
            return calculate_time_differences_from_db(conn, aweme_id)
    else:
        # 策略2: 直接从数据库获取
        return calculate_time_differences_from_db(conn, aweme_id)
    return df
```

**优势**:
- 数据质量优先：优先使用清洗后的高质量数据
- 容错机制：清洗数据不可用时自动降级
- 灵活性：支持指定视频或全量分析

#### 2.1.2 清洗数据加载算法
**算法名称**: `load_from_cleaned_data()`
**目的**: 从JSON格式的清洗数据文件中加载时间信息

**算法步骤**:
1. **文件存在性检查**: 验证清洗数据文件路径
2. **JSON数据解析**: 使用`json.load()`加载数据
3. **字段完整性验证**: 检查必要字段是否存在
4. **子评论过滤**: 筛选有效的子评论数据
5. **时间差计算**: 调用专门的时间差计算函数

**必要字段验证**:
```python
required_fields = [
    'comment_id',           # 评论ID
    'parent_comment_id',    # 父评论ID
    'create_time'           # 创建时间
]
```

#### 2.1.3 时间差计算算法（新增）
**算法名称**: `calculate_time_differences_from_data()`
**目的**: 从清洗数据计算子评论与父评论的时间差

**算法实现**:
```python
def calculate_time_differences_from_data(self, df):
    # 创建时间映射
    time_map = df.set_index('comment_id')['create_time'].to_dict()
    
    def get_time_diff(row):
        if pd.isna(row['parent_comment_id']) or row['parent_comment_id'] == '0':
            return None
        
        parent_time = time_map.get(row['parent_comment_id'])
        if parent_time:
            return row['create_time'] - parent_time
        return None
    
    # 计算时间差
    df['time_diff_seconds'] = df.apply(get_time_diff, axis=1)
    df['time_diff_minutes'] = df['time_diff_seconds'].apply(lambda x: x/60 if x is not None else None)
    
    # 过滤无效的时间差
    df = df[df['time_diff_minutes'] >= 0]
    df = df[df['time_diff_minutes'] <= 10080]  # 过滤超过1周的数据
    
    return df
```

**算法特点**:
- 独立的时间差计算逻辑
- 自动过滤异常值
- 支持批量处理

#### 2.1.4 数据库直接获取算法
**算法名称**: `calculate_time_differences_from_db()`
**目的**: 直接从MySQL数据库计算时间差

**SQL查询策略**:
```sql
SELECT 
    c_child.comment_id,
    c_parent.comment_id AS parent_comment_id,
    c_child.aweme_id,
    c_child.content AS child_content,
    c_parent.content AS parent_content,
    c_child.create_time AS child_time,
    c_parent.create_time AS parent_time,
    TIMESTAMPDIFF(MINUTE, 
        FROM_UNIXTIME(c_parent.create_time),
        FROM_UNIXTIME(c_child.create_time)
    ) AS time_diff_minutes,
    TIMESTAMPDIFF(SECOND, 
        FROM_UNIXTIME(c_parent.create_time),
        FROM_UNIXTIME(c_child.create_time)
    ) AS time_diff_seconds
FROM douyin_aweme_comment c_child
JOIN douyin_aweme_comment c_parent
    ON c_child.parent_comment_id = c_parent.comment_id
WHERE c_child.parent_comment_id != '0'
    AND c_child.parent_comment_id IS NOT NULL
ORDER BY c_parent.create_time, c_child.create_time
```

**数据过滤规则**:
- 时间差 >= 0（排除负值）
- 时间差 <= 10080分钟（排除超过1周的异常值）

### 2.2 时间分布分析算法

#### 2.2.1 基础统计分析
**算法名称**: `analyze_time_distribution()`
**目的**: 计算时间差的基础统计指标

**统计指标**:
```python
stats = {
    'count': len(time_diffs),                    # 样本数量
    'mean': time_diffs.mean(),                   # 平均值
    'median': time_diffs.median(),               # 中位数
    'std': time_diffs.std(),                     # 标准差
    'min': time_diffs.min(),                     # 最小值
    'max': time_diffs.max(),                     # 最大值
    'q25': time_diffs.quantile(0.25),           # 25%分位数
    'q75': time_diffs.quantile(0.75)            # 75%分位数
}
```

#### 2.2.2 时间窗口分布算法
**算法名称**: `calculate_window_distribution()`
**目的**: 计算各时间窗口的评论数量和占比

**算法实现**:
```python
for window_name, (min_time, max_time) in self.time_windows.items():
    if max_time is None:
        # 长期从众：超过1周
        mask = time_diffs >= min_time
    else:
        # 其他窗口
        mask = (time_diffs >= min_time) & (time_diffs < max_time)
    
    count = mask.sum()
    percentage = (count / len(time_diffs)) * 100
    
    window_counts[window_name] = count
    window_percentages[window_name] = percentage
```

**输出示例**:
```
immediate: 7 条 (3.2%) - 0-5分钟
quick: 15 条 (6.8%) - 5-30分钟
medium: 30 条 (13.5%) - 30-120分钟
slow: 117 条 (52.7%) - 120-1440分钟
delayed: 53 条 (23.9%) - 1440-10080分钟
```

### 2.3 从众行为检测算法

#### 2.3.1 关键时间点检测
**算法名称**: `detect_conformity_windows()`
**目的**: 识别从众行为的关键时间节点

**算法步骤**:
1. **分钟级统计**: 按分钟统计评论数量
2. **累计百分比计算**: 计算累计分布
3. **关键点识别**: 找到10%、25%、50%、75%、90%的时间点

**算法实现**:
```python
# 按分钟统计评论数量
minute_counts = time_diffs.value_counts().sort_index()

# 计算累计百分比
cumulative_percentage = (minute_counts.cumsum() / len(time_diffs)) * 100

# 找到关键时间点
for target_percentage in [10, 25, 50, 75, 90]:
    time_point = cumulative_percentage[cumulative_percentage >= target_percentage].index[0]
    key_points[f'{target_percentage}%'] = time_point
```

#### 2.3.2 密集回复时间段检测
**算法名称**: `find_dense_periods()`
**目的**: 检测评论密集回复的时间段

**算法参数**:
- `threshold_percentage`: 密集阈值（默认10%）
- `window_size`: 滑动窗口大小（默认5分钟）

**算法流程**:
```python
# 计算阈值
threshold_count = (threshold_percentage / 100) * total_comments

# 滑动窗口检测
for i in range(len(minute_counts) - window_size + 1):
    window_sum = minute_counts.iloc[i:i+window_size].sum()
    if window_sum >= threshold_count:
        # 记录密集时间段
        dense_periods.append({
            'start': start_minute,
            'end': end_minute,
            'count': window_sum,
            'percentage': (window_sum / total_comments) * 100
        })

# 合并相邻时间段
merged_periods = merge_adjacent_periods(dense_periods)
```

**合并策略**:
- 如果两个时间段间隔 ≤ 1分钟，则合并
- 合并后重新计算评论数量和占比

### 2.4 可视化算法（优化版）

#### 2.4.1 多子图布局算法
**算法名称**: `create_visualizations()`
**目的**: 创建综合性的时间分析可视化图表

**图表布局**:
```
┌─────────────────┬─────────────────┐
│   时间差直方图   │   累计分布曲线   │
│   (左上)        │   (右上)        │
├─────────────────┼─────────────────┤
│   时间窗口饼图   │   分钟级分布     │
│   (左下)        │   (右下)        │
└─────────────────┴─────────────────┘
```

#### 2.4.2 独立数据计算算法（新增）
**优化内容**: 可视化函数不再依赖`analysis_results`，独立计算所需数据

**算法实现**:
```python
# 累计分布曲线 - 独立计算
if len(time_diffs) > 0:
    minute_counts = time_diffs.value_counts().sort_index()
    cumulative_percentage = (minute_counts.cumsum() / len(time_diffs)) * 100
    
    ax2.plot(cumulative_percentage.index, cumulative_percentage.values, linewidth=2, color='green')
    
    # 标记关键点
    for target_percentage in [10, 25, 50, 75, 90]:
        try:
            time_point = cumulative_percentage[cumulative_percentage >= target_percentage].index[0]
            percentage = cumulative_percentage[time_point]
            ax2.scatter(time_point, percentage, color='red', s=50, zorder=5)
            ax2.annotate(f'{target_percentage}%\n{time_point}分钟', 
                       (time_point, percentage), 
                       xytext=(10, 10), textcoords='offset points',
                       bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
        except IndexError:
            continue

# 时间窗口分布 - 独立计算
window_counts = {}
for window_name, (min_time, max_time) in self.time_windows.items():
    if max_time is None:
        mask = time_diffs >= min_time
    else:
        mask = (time_diffs >= min_time) & (time_diffs < max_time)
    window_counts[window_name] = mask.sum()

# 分钟级分布 - 独立计算
if len(time_diffs) > 0:
    minute_counts = time_diffs.value_counts().sort_index()
    display_data = minute_counts.head(100)
    ax4.bar(display_data.index, display_data.values, alpha=0.7, color='lightcoral')
```

#### 2.4.3 时间差直方图算法
**算法实现**:
```python
ax1.hist(time_diffs, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
ax1.set_xlabel('时间差（分钟）')
ax1.set_ylabel('评论数量')
ax1.set_title('子评论回复时间分布')

# 添加统计线
mean_time = time_diffs.mean()
median_time = time_diffs.median()
ax1.axvline(mean_time, color='red', linestyle='--', label=f'平均值: {mean_time:.1f}分钟')
ax1.axvline(median_time, color='orange', linestyle='--', label=f'中位数: {median_time:.1f}分钟')
```

#### 2.4.4 累计分布曲线算法
**算法实现**:
```python
ax2.plot(cumulative_percentage.index, cumulative_percentage.values, linewidth=2, color='green')
ax2.set_xlabel('时间差（分钟）')
ax2.set_ylabel('累计百分比（%）')
ax2.set_title('累计回复时间分布')

# 标记关键点
for target_percentage in [10, 25, 50, 75, 90]:
    try:
        time_point = cumulative_percentage[cumulative_percentage >= target_percentage].index[0]
        percentage = cumulative_percentage[time_point]
        ax2.scatter(time_point, percentage, color='red', s=50, zorder=5)
        ax2.annotate(f'{target_percentage}%\n{time_point}分钟', 
                   (time_point, percentage), 
                   xytext=(10, 10), textcoords='offset points',
                   bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))
    except IndexError:
        continue
```

#### 2.4.5 时间窗口饼图算法
**算法实现**:
```python
# 只显示有数据的窗口
non_zero_data = [(name, count) for name, count in window_counts.items() if count > 0]
if non_zero_data:
    names, counts = zip(*non_zero_data)
    colors = plt.cm.Set3(np.linspace(0, 1, len(names)))
    
    wedges, texts, autotexts = ax3.pie(counts, labels=names, autopct='%1.1f%%', 
                                      colors=colors, startangle=90)
    ax3.set_title('时间窗口分布')
```

#### 2.4.6 分钟级分布柱状图算法
**算法实现**:
```python
# 只显示前100分钟的数据，避免图表过于密集
display_data = minute_counts.head(100)
if len(display_data) > 0:
    ax4.bar(display_data.index, display_data.values, alpha=0.7, color='lightcoral')
    ax4.set_xlabel('时间差（分钟）')
    ax4.set_ylabel('评论数量')
    ax4.set_title('分钟级评论数量分布（前100分钟）')
    ax4.grid(True, alpha=0.3)
```

### 2.5 报告生成算法

#### 2.5.1 数据类型转换算法
**算法名称**: `convert_numpy_types()`
**目的**: 将numpy数据类型转换为JSON可序列化的Python原生类型

**转换规则**:
```python
def convert_numpy_types(obj):
    if isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (np.integer, np.int64, np.int32)):
        return int(obj)
    elif isinstance(obj, (np.floating, np.float64, np.float32)):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif hasattr(obj, 'to_dict'):  # pandas Series
        return obj.to_dict()
    elif hasattr(obj, 'tolist'):  # pandas Index
        return obj.tolist()
    else:
        return obj
```

#### 2.5.2 报告结构生成算法
**算法名称**: `generate_report()`
**目的**: 生成结构化的分析报告

**报告结构**:
```python
report = {
    'analysis_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
    'data_summary': {
        'total_child_comments': int(len(df)),
        'valid_time_diffs': int(len(df['time_diff_minutes'].dropna())),
        'videos_analyzed': int(df['aweme_id'].nunique() if 'aweme_id' in df.columns else 1)
    },
    'time_statistics': convert_numpy_types(analysis_results.get('stats', {})),
    'window_distribution': convert_numpy_types(analysis_results.get('window_stats', {})),
    'conformity_windows': convert_numpy_types(analysis_results.get('conformity_windows', {})),
    'key_findings': []
}
```

## 三、算法复杂度分析

### 3.1 时间复杂度
- **数据加载**: O(n)，其中n为评论数量
- **时间差计算**: O(n)，单次遍历
- **统计分析**: O(n log n)，主要来自排序操作
- **窗口分布计算**: O(n × w)，其中w为时间窗口数量
- **密集时间段检测**: O(n × window_size)
- **可视化生成**: O(n)

### 3.2 空间复杂度
- **数据存储**: O(n)
- **统计分析**: O(n)
- **可视化**: O(n)

## 四、性能优化策略

### 4.1 数据库优化
- 使用索引优化JOIN查询
- 限制查询范围（时间、视频ID）
- 使用TIMESTAMPDIFF函数减少应用层计算

### 4.2 内存优化
- 分批处理大数据集
- 及时释放不需要的数据
- 使用pandas的向量化操作

### 4.3 计算优化
- 缓存重复计算结果
- 使用numpy的向量化操作
- 优化循环结构

## 五、错误处理机制

### 5.1 数据质量检查
- 文件存在性验证
- 字段完整性检查
- 数据类型验证
- 异常值过滤

### 5.2 降级策略
- 清洗数据不可用时自动切换到数据库
- 部分数据缺失时的容错处理
- 可视化失败时的备选方案

### 5.3 异常处理
```python
try:
    # 主要处理逻辑
    df = load_from_cleaned_data(cleaned_data_path)
except Exception as e:
    print(f"❌ 加载清洗数据失败: {e}")
    return None
```

## 六、使用示例

### 6.1 基本使用
```python
analyzer = ConformityTimeAnalyzer()
conn = get_db_conn()

# 分析所有视频
df = analyzer.calculate_time_differences(conn)
stats, window_stats = analyzer.analyze_time_distribution(df)
conformity_results = analyzer.detect_conformity_windows(df)

# 生成可视化
analyzer.create_visualizations(df, analysis_results)
analyzer.generate_report(df, analysis_results)
```

### 6.2 指定视频分析
```python
# 分析特定视频
aweme_id = "7504685138773593371"
df = analyzer.calculate_time_differences(conn, aweme_id=aweme_id)
```

### 6.3 使用清洗数据
```python
# 从清洗数据调取
cleaned_data_path = "../data/douyin_comments_processed.json"
df = analyzer.calculate_time_differences(
    conn, 
    use_cleaned_data=True, 
    cleaned_data_path=cleaned_data_path
)
```

## 七、输出文件说明

### 7.1 可视化文件
- **路径**: `../data/visualizations/conformity_time_analysis.png`
- **格式**: PNG，300 DPI
- **尺寸**: 15×12英寸
- **内容**: 4个子图的综合分析图表

### 7.2 报告文件
- **路径**: `../data/reports/conformity_time_analysis_report.json`
- **格式**: JSON
- **编码**: UTF-8
- **内容**: 结构化的分析结果和关键发现

## 八、重要更新说明

### 8.1 新增功能
**时间差计算模块**:
- ✅ 新增 `calculate_time_differences_from_data()` 函数
- ✅ 独立的时间差计算逻辑
- ✅ 支持从清洗数据计算时间差

### 8.2 可视化优化
**独立数据计算**:
- ✅ 可视化函数不再依赖 `analysis_results`
- ✅ 右侧图表问题已解决
- ✅ 所有4个子图都能正确显示

### 8.3 模块职责分离
**数据流程优化**:
```
清洗数据 → 时间分析模块 → 时间差计算 → 可视化报告
```

**优势**:
- 模块职责更清晰
- 代码复用性更好
- 维护性更强
- 扩展性更好

## 九、扩展性设计

### 9.1 模块化设计
- 各功能模块独立，便于维护和扩展
- 支持插件式的时间窗口定义
- 可配置的算法参数

### 9.2 多平台支持
- 数据库连接抽象化
- 文件格式标准化
- 可视化样式可配置

### 9.3 算法扩展
- 支持自定义时间窗口
- 可扩展的从众检测算法
- 灵活的可视化组件

## 十、总结

`conformity_time_analysis.py` 模块通过精心设计的算法架构，实现了从众心理时间分析的完整功能。该模块具有以下特点：

1. **算法完整性**: 覆盖数据获取、时间差计算、分析处理、可视化、报告生成全流程
2. **性能优化**: 采用多种优化策略，确保大数据集处理效率
3. **容错机制**: 完善的错误处理和降级策略
4. **扩展性**: 模块化设计，便于功能扩展和维护
5. **易用性**: 提供友好的用户界面和详细的使用文档

**重要更新**: 新增独立的时间差计算功能，优化可视化生成逻辑，解决了右侧图表显示问题，提高了模块的独立性和可靠性。

该模块为从众心理研究提供了强有力的技术支撑，能够有效识别和分析社交媒体中的从众行为时间特征。 